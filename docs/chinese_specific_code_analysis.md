# ä¸­æ–‡ç‰¹å®šä»£ç åˆ†æ - æ”¯æŒè‹±è¯­æ‰€éœ€çš„ä¿®æ”¹

## ğŸ“‹ åˆ†æç›®æ ‡

æ£€æŸ¥æ‰€æœ‰ä»£ç ä¸­ä¸“ç”¨äºä¸­æ–‡çš„éƒ¨åˆ†ï¼Œä»¥ä¾¿æ”¯æŒè‹±è¯­ç”Ÿæˆã€‚

---

## ğŸ” ä¸­æ–‡ç‰¹å®šä»£ç ä½ç½®

### 1. **Speculative Decoding ç”Ÿæˆé€»è¾‘** (`src/speculative_decoding.py`)

#### 1.1 CJKå­—ç¬¦æ£€æµ‹å‡½æ•°

**ä½ç½®**: ç¬¬929-935è¡Œ, 961-967è¡Œ

**ä»£ç **:
```python
def _is_cjk(token_id):
    """Check if token contains Chinese/Japanese/Korean characters"""
    try:
        s = tokenizer.decode([token_id], skip_special_tokens=True)
        return any('\u4e00' <= ch <= '\u9fff' for ch in s)  # âœ… CJKèŒƒå›´
    except:
        return False
```

**å½±å“çš„é€»è¾‘**ï¼š
1. **Repetition penalty** (ç¬¬942-945è¡Œ)ï¼šä»…å¯¹CJK tokenåº”ç”¨1.22çš„æƒ©ç½š
2. **Same-char blocking** (ç¬¬984-986è¡Œ)ï¼šä»…é˜»æ­¢CJKå­—ç¬¦çš„immediateé‡å¤
3. **Content-only trigram** (ç¬¬992-1004è¡Œ)ï¼šä»…å½“recent windowå…¨æ˜¯CJKæ—¶åº”ç”¨

**é—®é¢˜**ï¼š
- âŒ å¯¹è‹±è¯­tokenä¼šè¿”å›False
- âŒ è‹±è¯­ä¸ä¼šåº”ç”¨repetition penaltyã€same-char blockingã€trigram ban

---

#### 1.2 ä¸­æ–‡æ ‡ç‚¹ç¬¦å·

**ä½ç½®**: ç¬¬979, 1009-1010è¡Œ

**ä»£ç **:
```python
# ç¬¬979è¡Œ
PUNCT_IDS = _ids_for(['ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', ':', 'ï¼›', 'ï¼', 'ï¼Ÿ'])
                    # â†‘ åªæœ‰ä¸­æ–‡æ ‡ç‚¹

# ç¬¬1009-1010è¡Œ
COMMA_LIKE = _ids_for(['ï¼Œ', 'ã€', 'ï¼š', ':'])  # ä¸­æ–‡é€—å·ç±»
PERIOD_LIKE = _ids_for(['ã€‚'])  # ä¸­æ–‡å¥å·
```

**é—®é¢˜**ï¼š
- âŒ **æ²¡æœ‰è‹±æ–‡æ ‡ç‚¹**ï¼š`,`, `.`, `;`, `!`, `?`
- âŒ è‹±è¯­ç”Ÿæˆæ—¶ï¼Œæ ‡ç‚¹é—¸é—¨æ— æ•ˆ

---

#### 1.3 æ ‡ç‚¹é—¸é—¨ï¼ˆPunctuation Gateï¼‰

**ä½ç½®**: ç¬¬1012-1038è¡Œ

**ä»£ç **:
```python
# ç»Ÿè®¡è‡ªä¸Šæ¬¡æ ‡ç‚¹ä»¥æ¥çš„CJKå­—ç¬¦æ•°
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    s = tokenizer.decode([t])
    if any('\u4e00' <= ch <= '\u9fff' for ch in s):  # âŒ åªç»Ÿè®¡CJK
        since_punct += 1

# é€—å·é—¸é—¨ï¼šè¦æ±‚è‡³å°‘4ä¸ªCJKå­—ç¬¦
if since_punct < 4:
    logits[COMMA_LIKE] = -inf

# å¥å·é—¸é—¨ï¼šè¦æ±‚è‡³å°‘5ä¸ªCJKå­—ç¬¦  
if since_punct < 5:
    logits[PERIOD_LIKE] -= 3.5
```

**é—®é¢˜**ï¼š
- âŒ åªç»Ÿè®¡CJKå­—ç¬¦ï¼Œä¸ç»Ÿè®¡è‹±è¯­å•è¯/token
- âŒ é˜ˆå€¼ï¼ˆ4å­—ï¼Œ5å­—ï¼‰æ˜¯é’ˆå¯¹ä¸­æ–‡è®¾è®¡çš„
- âŒ è‹±è¯­ç”Ÿæˆæ—¶åŸºæœ¬ä¸èµ·ä½œç”¨

---

#### 1.4 å¥å­ç»“æŸåˆ¤æ–­

**ä½ç½®**: ç¬¬158-160è¡Œ

**ä»£ç **:
```python
sentence_endings = ['ã€‚', '.']  # âœ… åŒ…å«ä¸­è‹±æ–‡
return token_text in sentence_endings
```

**çŠ¶æ€**: âœ… **å·²æ”¯æŒè‹±è¯­**

---

### 2. **Stopping Criteria** (`src/models/stopping_criteria.py`)

**ä½ç½®**: ç¬¬17, 134è¡Œ

**ä»£ç **:
```python
# ç¬¬17è¡Œ
sentence_end_chars=("ã€‚", ".")  # âœ… é»˜è®¤åŒ…å«ä¸­è‹±æ–‡

# ç¬¬134è¡Œ
sentence_end_chars=("ã€‚", ".")  # âœ… é»˜è®¤åŒ…å«ä¸­è‹±æ–‡
```

**çŠ¶æ€**: âœ… **å·²æ”¯æŒè‹±è¯­**

---

### 3. **BLEUè®¡ç®—** (æ‰€æœ‰baselineè„šæœ¬)

**ä½ç½®**: 
- `run_edge_baseline_cpu_limited.py` ç¬¬535è¡Œ
- `run_cloud_baseline.py` ç¬¬265è¡Œ
- `run_speculative_decoding_cpu_limited.py` ç¬¬573è¡Œ

**ä»£ç **:
```python
# ç¬¬535è¡Œï¼ˆedge baselineï¼‰
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize='zh')  # âŒ ä¸­æ–‡tokenization
overall_bleu = corpus_bleu.score / 100.0

# ç»“æœkey
"corpus_bleu_zh": overall_bleu  # âŒ å›ºå®šä½¿ç”¨"_zh"
```

**é—®é¢˜**ï¼š
- âŒ **ç¡¬ç¼–ç ä½¿ç”¨ä¸­æ–‡tokenization** (`tokenize='zh'`)
- âŒ è‹±è¯­åº”è¯¥ä½¿ç”¨ `tokenize='13a'` æˆ– `tokenize='intl'`
- âŒ ç»“æœkeyåç§°å›ºå®šä¸º `corpus_bleu_zh`

---

### 4. **Promptæ¨¡æ¿** (æ‰€æœ‰baselineè„šæœ¬)

**ä½ç½®**: 
- `run_edge_baseline_cpu_limited.py` ç¬¬316-350è¡Œ
- `run_cloud_baseline.py` ç¬¬61-114è¡Œ
- `run_speculative_decoding_cpu_limited.py` ç¬¬370-421è¡Œ

**ä»£ç ç¤ºä¾‹** (`run_cloud_baseline.py`):
```python
def get_prompt_template(prompt_type: str, language: str) -> str:
    """Get prompt template based on prompt type and language"""
    
    if prompt_type == "default":
        if language == "chinese":
            return """..."""  # âœ… ä¸­æ–‡prompt
        elif language == "english":
            return "Please generate a concise English emotion description..."  # âœ… è‹±æ–‡prompt
    
    elif prompt_type == "detailed":
        if language == "chinese":
            return """ä»»åŠ¡ï¼šè¯·ç”Ÿæˆ"æƒ…æ„Ÿè¯´æ˜é•¿å¥"..."""  # âœ… ä¸­æ–‡è¯¦ç»†prompt
        elif language == "english":
            return "Please provide a detailed analysis..."  # âœ… è‹±æ–‡è¯¦ç»†prompt
```

**çŠ¶æ€**: âœ… **å·²æ”¯æŒè‹±è¯­** (é€šè¿‡`language`å‚æ•°åˆ‡æ¢)

---

## ğŸ“Š é—®é¢˜æ±‡æ€»

### âŒ éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†

| ä½ç½® | é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **1. Spec Decoding - CJKæ£€æµ‹** | è‹±è¯­ä¸åº”ç”¨æŸäº›çº¦æŸ | Repetition/n-gramæ§åˆ¶ | ğŸ”´ é«˜ |
| **2. Spec Decoding - æ ‡ç‚¹åˆ—è¡¨** | ç¼ºå°‘è‹±æ–‡æ ‡ç‚¹ | æ ‡ç‚¹é—¸é—¨å¤±æ•ˆ | ğŸ”´ é«˜ |
| **3. Spec Decoding - æ ‡ç‚¹é—¸é—¨** | åªç»Ÿè®¡CJKå­—ç¬¦ | è‹±è¯­æ ‡ç‚¹æ§åˆ¶å¤±æ•ˆ | ğŸ”´ é«˜ |
| **4. BLEUè®¡ç®—** | ç¡¬ç¼–ç `tokenize='zh'` | è‹±è¯­BLEUä¸å‡†ç¡® | ğŸŸ¡ ä¸­ |
| **5. BLEUç»“æœkey** | ç¡¬ç¼–ç `corpus_bleu_zh` | å‘½åä¸ç»Ÿä¸€ | ğŸŸ¢ ä½ |

### âœ… å·²æ”¯æŒè‹±è¯­çš„éƒ¨åˆ†

| ä½ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| **Sentence endings** | âœ… | åŒ…å«`.` |
| **Stopping criteria** | âœ… | æ”¯æŒ`("ã€‚", ".")` |
| **Prompt templates** | âœ… | é€šè¿‡`language`å‚æ•°åˆ‡æ¢ |

---

## ğŸ’¡ ä¿®æ”¹å»ºè®®

### æ–¹æ¡ˆA: è¯­è¨€æ„ŸçŸ¥ï¼ˆLanguage-Awareï¼‰ã€æ¨èã€‘

åœ¨æ‰€æœ‰éœ€è¦çš„åœ°æ–¹æ·»åŠ è¯­è¨€æ£€æµ‹ï¼Œæ ¹æ®è¯­è¨€åº”ç”¨ä¸åŒé€»è¾‘ã€‚

#### ä¼˜ç‚¹ï¼š
- âœ… æœ€ç²¾ç¡®æ§åˆ¶
- âœ… ä¸­è‹±æ–‡éƒ½èƒ½è·å¾—æœ€ä¼˜çº¦æŸ
- âœ… æœªæ¥æ˜“äºæ‰©å±•å…¶ä»–è¯­è¨€

#### ç¼ºç‚¹ï¼š
- âš ï¸ éœ€è¦ä¼ é€’`language`å‚æ•°åˆ°ç”Ÿæˆå‡½æ•°
- âš ï¸ ä»£ç å¤æ‚åº¦ç¨é«˜

---

### æ–¹æ¡ˆB: è¯­è¨€æ— å…³ï¼ˆLanguage-Agnosticï¼‰

ç§»é™¤æ‰€æœ‰CJKç‰¹å®šé€»è¾‘ï¼Œä½¿ç”¨é€šç”¨çº¦æŸã€‚

#### ä¼˜ç‚¹ï¼š
- âœ… ä»£ç ç®€å•
- âœ… è‡ªåŠ¨æ”¯æŒæ‰€æœ‰è¯­è¨€

#### ç¼ºç‚¹ï¼š
- âŒ å¯èƒ½æ— æ³•è§£å†³ä¸­æ–‡ç‰¹æœ‰é—®é¢˜ï¼ˆæ ‡ç‚¹æ³›æ»¥ç­‰ï¼‰
- âŒ æ€§èƒ½å¯èƒ½ä¸‹é™

---

### æ–¹æ¡ˆC: æ··åˆæ–¹æ¡ˆã€æ¨èå®æ–½ã€‘

**æ ¸å¿ƒæ€æƒ³**ï¼šä¿ç•™å¯¹è´¨é‡å½±å“å¤§çš„è¯­è¨€ç‰¹å®šé€»è¾‘ï¼Œå…¶ä»–éƒ¨åˆ†ä½¿ç”¨é€šç”¨é€»è¾‘ã€‚

#### éœ€è¦ä¿®æ”¹çš„ä¼˜å…ˆçº§ï¼š

##### ğŸ”´ **Priority 1: BLEUè®¡ç®—**ï¼ˆå¿…é¡»æ”¹ï¼‰

**ä½ç½®**: æ‰€æœ‰baselineè„šæœ¬çš„corpus_bleuè®¡ç®—

**ä¿®æ”¹**:
```python
# ä¿®æ”¹å‰
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize='zh')  âŒ

# ä¿®æ”¹å
bleu_tokenize = 'zh' if language == 'chinese' else '13a'
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize=bleu_tokenize)  âœ…

# ç»“æœkeyä¹Ÿæ”¹ä¸ºåŠ¨æ€
bleu_key = f"corpus_bleu_{language[:2]}"  # "corpus_bleu_zh" or "corpus_bleu_en"
```

**åŸå› **: è‹±è¯­ä½¿ç”¨ä¸­æ–‡tokenizationä¼šå¯¼è‡´BLEUåˆ†æ•°ä¸å‡†ç¡®ã€‚

---

##### ğŸŸ¡ **Priority 2: Speculative Decodingæ ‡ç‚¹åˆ—è¡¨**ï¼ˆå»ºè®®æ”¹ï¼‰

**ä½ç½®**: `src/speculative_decoding.py` ç¬¬979, 1009-1010è¡Œ

**ä¿®æ”¹**:
```python
# ä¿®æ”¹å‰ï¼ˆåªæœ‰ä¸­æ–‡ï¼‰
PUNCT_IDS = _ids_for(['ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', ':', 'ï¼›', 'ï¼', 'ï¼Ÿ'])
COMMA_LIKE = _ids_for(['ï¼Œ', 'ã€', 'ï¼š', ':'])
PERIOD_LIKE = _ids_for(['ã€‚'])

# ä¿®æ”¹åï¼ˆä¸­è‹±æ–‡éƒ½æœ‰ï¼‰
PUNCT_IDS = _ids_for([
    'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'ï¼›', 'ï¼', 'ï¼Ÿ',  # ä¸­æ–‡
    ',', '.', ';', ':', '!', '?'              # è‹±æ–‡
])
COMMA_LIKE = _ids_for([
    'ï¼Œ', 'ã€', 'ï¼š',  # ä¸­æ–‡
    ',', ';', ':'      # è‹±æ–‡
])
PERIOD_LIKE = _ids_for([
    'ã€‚',  # ä¸­æ–‡
    '.'    # è‹±æ–‡
])
```

**åŸå› **: è‹±è¯­ç”Ÿæˆæ—¶éœ€è¦æ ‡ç‚¹é—¸é—¨é˜²æ­¢æ ‡ç‚¹æ³›æ»¥ã€‚

---

##### ğŸŸ¡ **Priority 3: æ ‡ç‚¹é—¸é—¨çš„å­—ç¬¦ç»Ÿè®¡**ï¼ˆå»ºè®®æ”¹ï¼‰

**ä½ç½®**: `src/speculative_decoding.py` ç¬¬1012-1023è¡Œ

**ä¿®æ”¹**:
```python
# ä¿®æ”¹å‰ï¼ˆåªç»Ÿè®¡CJKï¼‰
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    s = tokenizer.decode([t])
    if any('\u4e00' <= ch <= '\u9fff' for ch in s):  # âŒ åªç»Ÿè®¡CJK
        since_punct += 1

# ä¿®æ”¹åï¼ˆç»Ÿè®¡æ‰€æœ‰éæ ‡ç‚¹tokenï¼‰
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    # ç»Ÿè®¡æ‰€æœ‰éæ ‡ç‚¹tokenï¼ˆä¸é™è¯­è¨€ï¼‰
    since_punct += 1  # âœ… é€šç”¨
```

**åŒæ—¶è°ƒæ•´é˜ˆå€¼**:
```python
# ä¸­æ–‡ï¼š4å­—=4tokens, 5å­—=5tokens
# è‹±æ–‡ï¼š4-5è¯ â‰ˆ 4-5tokensï¼ˆè‹±è¯­tokenizationé€šå¸¸1è¯=1-2tokensï¼‰

# é€—å·ï¼šä¿æŒ4 tokensï¼ˆè‹±è¯­çº¦2-4ä¸ªè¯ï¼‰
if since_punct < 4:
    logits[COMMA_LIKE] = -inf

# å¥å·ï¼šä¿æŒ5 tokensï¼ˆè‹±è¯­çº¦3-5ä¸ªè¯ï¼‰
if since_punct < 5:
    logits[PERIOD_LIKE] -= 3.5
```

**åŸå› **: é˜ˆå€¼è®¾è®¡è¾ƒåˆç†ï¼Œè‹±è¯­ä¹Ÿé€‚ç”¨ã€‚

---

##### ğŸŸ¢ **Priority 4: CJKç‰¹å®šçº¦æŸ**ï¼ˆå¯é€‰æ”¹ï¼‰

**ä½ç½®**: `src/speculative_decoding.py` ç¬¬926-1004è¡Œ

**é€‰é¡¹1: ä¿æŒä¸å˜**ï¼ˆæ¨èï¼‰
- Repetition penaltyã€same-char blockingã€trigram banåªå¯¹CJKç”Ÿæ•ˆ
- è‹±è¯­ä¸åº”ç”¨è¿™äº›çº¦æŸï¼ˆè‹±è¯­tokenizationä¸åŒï¼Œä¸éœ€è¦ï¼‰
- **ä¼˜ç‚¹**: ç®€å•ï¼Œä¸”è‹±è¯­baselineæœ¬æ¥å°±æ²¡è¿™äº›çº¦æŸ

**é€‰é¡¹2: æ”¹ä¸ºè¯­è¨€æ— å…³**
- æ‰€æœ‰è¯­è¨€éƒ½åº”ç”¨è¿™äº›çº¦æŸ
- éœ€è¦è°ƒæ•´å‚æ•°ï¼ˆå¦‚repetition_penaltyå¼ºåº¦ï¼‰
- **ç¼ºç‚¹**: å¯èƒ½å½±å“ä¸­æ–‡è´¨é‡

**æ¨è**: ä¿æŒä¸å˜ï¼Œè‹±è¯­ä¸éœ€è¦è¿™äº›CJKç‰¹å®šçš„çº¦æŸã€‚

---

## ğŸ¯ æœ€å°ä¿®æ”¹æ–¹æ¡ˆï¼ˆæ¨èå®æ–½ï¼‰

åªä¿®æ”¹**Priority 1å’Œ2**ï¼Œä¿æŒå…¶ä»–ä¸å˜ï¼š

### ä¿®æ”¹1: BLEUè®¡ç®—

**æ–‡ä»¶**: 
- `experiments/runs/run_edge_baseline_cpu_limited.py`
- `experiments/runs/run_cloud_baseline.py`
- `experiments/runs/run_speculative_decoding_cpu_limited.py`

**ä¿®æ”¹å†…å®¹**:
```python
# åœ¨corpus_bleuè®¡ç®—å¤„
bleu_tokenize = 'zh' if language == 'chinese' else '13a'
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize=bleu_tokenize)

# ç»“æœkeyæ”¹ä¸ºåŠ¨æ€
bleu_key = f"corpus_bleu_{language[:2]}"
metrics[bleu_key] = overall_bleu

# æ—¥å¿—ä¹Ÿæ”¹ä¸ºåŠ¨æ€
logger.info(f"Corpus BLEU ({language} tokenization): {overall_bleu:.4f}")
```

---

### ä¿®æ”¹2: Speculative Decodingæ ‡ç‚¹åˆ—è¡¨

**æ–‡ä»¶**: `src/speculative_decoding.py`

**ä¿®æ”¹å†…å®¹**:
```python
# ç¬¬979è¡Œ
PUNCT_IDS = _ids_for([
    'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'ï¼›', 'ï¼', 'ï¼Ÿ',  # Chinese
    ',', '.', ';', ':', '!', '?'              # English
])

# ç¬¬1009-1010è¡Œ
COMMA_LIKE = _ids_for(['ï¼Œ', 'ã€', 'ï¼š', ',', ';', ':'])
PERIOD_LIKE = _ids_for(['ã€‚', '.'])
```

---

### ä¿®æ”¹3: æ ‡ç‚¹é—¸é—¨ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰

**æ–‡ä»¶**: `src/speculative_decoding.py`

**ä¿®æ”¹å†…å®¹**:
```python
# ç¬¬1012-1023è¡Œï¼šç®€åŒ–ä¸ºç»Ÿè®¡æ‰€æœ‰éæ ‡ç‚¹token
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    since_punct += 1  # ä¸é™è¯­è¨€
```

---

## ğŸ§ª æµ‹è¯•å»ºè®®

### æµ‹è¯•1: è‹±è¯­Edge Baseline

```bash
python experiments/runs/run_edge_baseline_cpu_limited.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --max_cpu_cores 2 \
    --max_memory_gb 16.0 \
    --output_name edge_cpu_limited_secap_en
```

### æµ‹è¯•2: è‹±è¯­Cloud Baseline

```bash
python experiments/runs/run_cloud_baseline.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --output_name cloud_secap_en
```

### éªŒè¯ç‚¹

- [ ] BLEUä½¿ç”¨è‹±è¯­tokenization (`tokenize='13a'`)
- [ ] ç»“æœkeyä¸º `corpus_bleu_en`
- [ ] ç”Ÿæˆçš„è‹±è¯­æ–‡æœ¬æµç•…ï¼Œæ— æ ‡ç‚¹æ³›æ»¥
- [ ] Stopping criteriaæ­£ç¡®ï¼ˆæ£€æµ‹åˆ°`.`ååœæ­¢ï¼‰
- [ ] è¾“å‡ºé•¿åº¦åˆç†ï¼ˆ2-3å¥è¯ï¼‰

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒé—®é¢˜

ä»£ç ä¸­æœ‰**3ä¸ªä¸»è¦çš„ä¸­æ–‡ç‰¹å®šéƒ¨åˆ†**ï¼š
1. âœ… **Prompt**: å·²æ”¯æŒï¼ˆé€šè¿‡`language`å‚æ•°ï¼‰
2. âŒ **BLEUè®¡ç®—**: ç¡¬ç¼–ç ä¸­æ–‡tokenization
3. âŒ **æ ‡ç‚¹åˆ—è¡¨**: åªæœ‰ä¸­æ–‡æ ‡ç‚¹

### æ¨èæ–¹æ¡ˆ

**æœ€å°ä¿®æ”¹**ï¼ˆä¿®æ”¹1+2ï¼‰ï¼š
- ä¿®æ”¹BLEU tokenizationæ ¹æ®languageåŠ¨æ€é€‰æ‹©
- æ‰©å±•æ ‡ç‚¹åˆ—è¡¨åŒ…å«è‹±æ–‡æ ‡ç‚¹
- ä¿æŒCJKç‰¹å®šçº¦æŸä¸å˜ï¼ˆè‹±è¯­ä¸å—å½±å“ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- âœ… ä¸­æ–‡baselineç»§ç»­æ­£å¸¸å·¥ä½œ
- âœ… è‹±è¯­baselineèƒ½å¤Ÿæ­£ç¡®è¿è¡Œ
- âœ… è‹±è¯­BLEUåˆ†æ•°å‡†ç¡®
- âœ… è‹±è¯­ç”Ÿæˆè´¨é‡åˆç†

**å·¥ä½œé‡**: ä¿®æ”¹3ä¸ªæ–‡ä»¶ï¼Œçº¦20è¡Œä»£ç 
