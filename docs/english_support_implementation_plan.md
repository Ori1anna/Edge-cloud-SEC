# è‹±è¯­æ”¯æŒå®æ–½è®¡åˆ’ - å®Œæ•´åˆ†æä¸ä¿®æ”¹æ–¹æ¡ˆ

## ğŸ“‹ åˆ†æç»“æœæ€»ç»“

### âœ… å·²ç»æ”¯æŒè‹±è¯­çš„éƒ¨åˆ†

| ç»„ä»¶ | ä½ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|------|
| **Promptæ¨¡æ¿** | æ‰€æœ‰baselineè„šæœ¬ | âœ… | é€šè¿‡`--language english`åˆ‡æ¢ |
| **BERTScore** | `src/evaluation/metrics.py` | âœ… | ä¸­æ–‡ç”¨`bert-base-chinese`<br>è‹±æ–‡ç”¨`roberta-large` |
| **Stopping criteria** | `src/models/stopping_criteria.py` | âœ… | æ”¯æŒ`("ã€‚", ".")` |
| **Sentence endings** | `src/speculative_decoding.py` | âœ… | æ£€æµ‹`.`å’Œ`ã€‚` |

### âŒ éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†

| ç»„ä»¶ | ä½ç½® | é—®é¢˜ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **BLEU tokenization** | 3ä¸ªbaselineè„šæœ¬ | ç¡¬ç¼–ç `tokenize='zh'` | ğŸ”´ å¿…é¡» |
| **æ ‡ç‚¹ç¬¦å·åˆ—è¡¨** | `speculative_decoding.py` | åªæœ‰ä¸­æ–‡æ ‡ç‚¹ | ğŸŸ¡ å»ºè®® |
| **æ ‡ç‚¹é—¸é—¨ç»Ÿè®¡** | `speculative_decoding.py` | åªç»Ÿè®¡CJKå­—ç¬¦ | ğŸŸ¡ å»ºè®® |
| **CJKç‰¹å®šçº¦æŸ** | `speculative_decoding.py` | Repetition/n-gramåªå¯¹CJK | ğŸŸ¢ ä¿æŒä¸å˜ |

---

## ğŸ”§ è¯¦ç»†ä¿®æ”¹è®¡åˆ’

### ä¿®æ”¹1: BLEU Tokenizationï¼ˆğŸ”´ å¿…é¡»ä¿®æ”¹ï¼‰

#### å½±å“æ–‡ä»¶ï¼ˆ3ä¸ªï¼‰
1. `experiments/runs/run_edge_baseline_cpu_limited.py`
2. `experiments/runs/run_cloud_baseline.py`
3. `experiments/runs/run_speculative_decoding_cpu_limited.py`

#### å½“å‰ä»£ç 

**Edge Baseline** (ç¬¬535è¡Œ):
```python
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize='zh')  âŒ
overall_bleu = corpus_bleu.score / 100.0

"corpus_bleu_zh": overall_bleu  âŒ
logger.info(f"Corpus BLEU (Chinese tokenization): {overall_bleu:.4f}")  âŒ
```

**Cloud Baseline** (ç¬¬265è¡Œ): ç›¸åŒé—®é¢˜

**Spec Decoding** (ç¬¬573è¡Œ): ç›¸åŒé—®é¢˜

#### ä¿®æ”¹åä»£ç 

```python
# åŠ¨æ€é€‰æ‹©tokenization
bleu_tokenize = 'zh' if language == 'chinese' else '13a'
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize=bleu_tokenize)  âœ…
overall_bleu = corpus_bleu.score / 100.0

# åŠ¨æ€keyåç§°
bleu_key = f"corpus_bleu_{language[:2]}"  # "zh" or "en"
{bleu_key: overall_bleu}  âœ…

# åŠ¨æ€æ—¥å¿—
lang_display = "Chinese" if language == 'chinese' else "English"
logger.info(f"Corpus BLEU ({lang_display} tokenization): {overall_bleu:.4f}")  âœ…
```

**BERTScoreè¯´æ˜**ï¼ˆè§READMEï¼‰:
- ä¸­æ–‡ï¼š`tokenize='zh'` - ä½¿ç”¨ä¸­æ–‡åˆ†è¯
- è‹±æ–‡ï¼š`tokenize='13a'` - ä½¿ç”¨æ ‡å‡†è‹±æ–‡tokenizationï¼ˆå¤„ç†æ ‡ç‚¹ã€å¤§å°å†™ç­‰ï¼‰

---

### ä¿®æ”¹2: æ ‡ç‚¹ç¬¦å·åˆ—è¡¨ï¼ˆğŸŸ¡ å»ºè®®ä¿®æ”¹ï¼‰

#### å½±å“æ–‡ä»¶
- `src/speculative_decoding.py`

#### å½“å‰ä»£ç ï¼ˆç¬¬979, 1009-1010è¡Œï¼‰

```python
PUNCT_IDS = _ids_for(['ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', ':', 'ï¼›', 'ï¼', 'ï¼Ÿ'])  âŒ åªæœ‰ä¸­æ–‡
COMMA_LIKE = _ids_for(['ï¼Œ', 'ã€', 'ï¼š', ':'])  âŒ
PERIOD_LIKE = _ids_for(['ã€‚'])  âŒ
```

**é—®é¢˜**ï¼šè‹±è¯­ç”Ÿæˆæ—¶ï¼Œæ ‡ç‚¹é—¸é—¨ä¸èµ·ä½œç”¨ï¼ˆå› ä¸ºè‹±æ–‡æ ‡ç‚¹ä¸åœ¨åˆ—è¡¨ä¸­ï¼‰

#### ä¿®æ”¹åä»£ç 

```python
# åŒ…å«ä¸­è‹±æ–‡æ ‡ç‚¹
PUNCT_IDS = _ids_for([
    # Chinese punctuation
    'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'ï¼›', 'ï¼', 'ï¼Ÿ',
    # English punctuation
    ',', '.', ';', ':', '!', '?'
])

COMMA_LIKE = _ids_for([
    # Chinese
    'ï¼Œ', 'ã€', 'ï¼š',
    # English
    ',', ';', ':'
])

PERIOD_LIKE = _ids_for([
    # Chinese
    'ã€‚',
    # English
    '.'
])
```

**æ•ˆæœ**ï¼š
- âœ… ä¸­æ–‡å’Œè‹±æ–‡éƒ½èƒ½åº”ç”¨æ ‡ç‚¹é—¸é—¨
- âœ… é˜²æ­¢è‹±è¯­ä¹Ÿå‡ºç°é€—å·æ³›æ»¥

---

### ä¿®æ”¹3: æ ‡ç‚¹é—¸é—¨å­—ç¬¦ç»Ÿè®¡ï¼ˆğŸŸ¡ å»ºè®®ä¿®æ”¹ï¼‰

#### å½±å“æ–‡ä»¶
- `src/speculative_decoding.py`

#### å½“å‰ä»£ç ï¼ˆç¬¬1012-1023è¡Œï¼‰

```python
# åªç»Ÿè®¡CJKå­—ç¬¦
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    s = tokenizer.decode([t])
    if any('\u4e00' <= ch <= '\u9fff' for ch in s):  # âŒ åªç»Ÿè®¡CJK
        since_punct += 1
```

**é—®é¢˜**ï¼šè‹±è¯­tokenä¸ä¼šè¢«ç»Ÿè®¡ï¼Œsince_punctå§‹ç»ˆä¸º0ï¼Œæ ‡ç‚¹é—¸é—¨ä¸èµ·ä½œç”¨

#### ä¿®æ”¹åä»£ç 

```python
# ç»Ÿè®¡æ‰€æœ‰éæ ‡ç‚¹tokenï¼ˆä¸é™è¯­è¨€ï¼‰
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    # ç»Ÿè®¡æ‰€æœ‰tokenï¼ˆä¸­è‹±æ–‡é€šç”¨ï¼‰
    since_punct += 1  âœ…
```

**é˜ˆå€¼åˆ†æ**ï¼š
- ä¸­æ–‡ï¼š4ä¸ªtoken = 4ä¸ªæ±‰å­— = åˆç†
- è‹±æ–‡ï¼š4ä¸ªtoken â‰ˆ 2-4ä¸ªå•è¯ = åˆç†ï¼ˆè‹±è¯­tokenizationé€šå¸¸1è¯=1-2 tokensï¼‰
- **ç»“è®º**ï¼šé˜ˆå€¼æ— éœ€è°ƒæ•´ï¼Œå¯¹ä¸­è‹±æ–‡éƒ½é€‚ç”¨

---

### ä¿®æ”¹4: CJKç‰¹å®šçº¦æŸï¼ˆğŸŸ¢ ä¿æŒä¸å˜ï¼‰

#### ä»£ç ä½ç½®
- Repetition penalty (ç¬¬926-945è¡Œ)
- Same-char blocking (ç¬¬981-986è¡Œ)
- Content-only trigram (ç¬¬992-1004è¡Œ)

#### å†³å®šï¼š**ä¸ä¿®æ”¹**

**åŸå› **ï¼š
1. è¿™äº›çº¦æŸæ˜¯ä¸ºäº†è§£å†³**ä¸­æ–‡ç‰¹æœ‰é—®é¢˜**ï¼ˆæ ‡ç‚¹æ³›æ»¥ã€å•å­—é‡å¤ï¼‰
2. è‹±è¯­tokenizationä¸åŒï¼ˆé€šå¸¸1è¯=1 tokenï¼‰ï¼Œä¸ä¼šæœ‰è¿™äº›é—®é¢˜
3. è‹±è¯­ä¼šè‡ªåŠ¨è·³è¿‡è¿™äº›çº¦æŸï¼ˆ`_is_cjk()`è¿”å›Falseï¼‰

**éªŒè¯**ï¼š
- è‹±è¯­tokençš„`_is_cjk()`æ£€æµ‹ä¼šè¿”å›False
- Repetition penaltyä¸ä¼šåº”ç”¨åˆ°è‹±è¯­
- Same-char blockingä¸ä¼šåº”ç”¨åˆ°è‹±è¯­
- Trigram banä¸ä¼šåº”ç”¨åˆ°è‹±è¯­
- **è¿™æ˜¯æ­£ç¡®çš„è¡Œä¸ºï¼**

---

## ğŸ“Š ä¿®æ”¹æ€»ç»“è¡¨

| ä¿®æ”¹é¡¹ | æ–‡ä»¶ | è¡Œå· | ä¼˜å…ˆçº§ | å·¥ä½œé‡ |
|--------|------|------|--------|--------|
| **BLEU tokenize** | `run_edge_baseline_cpu_limited.py` | 535 | ğŸ”´ å¿…é¡» | 5è¡Œ |
| **BLEU tokenize** | `run_cloud_baseline.py` | 265 | ğŸ”´ å¿…é¡» | 5è¡Œ |
| **BLEU tokenize** | `run_speculative_decoding_cpu_limited.py` | 573 | ğŸ”´ å¿…é¡» | 5è¡Œ |
| **æ ‡ç‚¹åˆ—è¡¨** | `speculative_decoding.py` | 979, 1009-1010 | ğŸŸ¡ å»ºè®® | 10è¡Œ |
| **é—¸é—¨ç»Ÿè®¡** | `speculative_decoding.py` | 1012-1023 | ğŸŸ¡ å»ºè®® | 5è¡Œ |

**æ€»å·¥ä½œé‡**: çº¦30è¡Œä»£ç ä¿®æ”¹

---

## ğŸ¯ å®æ–½é¡ºåº

### Phase 1: BLEUä¿®å¤ï¼ˆå¿…é¡»ï¼Œç«‹å³å®æ–½ï¼‰

ä¿®æ”¹3ä¸ªbaselineè„šæœ¬çš„BLEUè®¡ç®—ï¼Œä½¿å…¶æ ¹æ®languageå‚æ•°åŠ¨æ€é€‰æ‹©tokenizationã€‚

**é¢„æœŸæ•ˆæœ**ï¼š
- âœ… è‹±è¯­BLEUåˆ†æ•°å‡†ç¡®
- âœ… ä¸­æ–‡BLEUä¸å—å½±å“

---

### Phase 2: æ ‡ç‚¹æ§åˆ¶ï¼ˆå»ºè®®ï¼Œå¯é€‰ï¼‰

ä¿®æ”¹Speculative Decodingçš„æ ‡ç‚¹åˆ—è¡¨å’Œç»Ÿè®¡é€»è¾‘ã€‚

**é¢„æœŸæ•ˆæœ**ï¼š
- âœ… è‹±è¯­ä¹Ÿèƒ½åº”ç”¨æ ‡ç‚¹é—¸é—¨
- âœ… é˜²æ­¢è‹±è¯­æ ‡ç‚¹æ³›æ»¥

---

### Phase 3: æµ‹è¯•éªŒè¯

è¿è¡Œè‹±è¯­baselineéªŒè¯ä¿®æ”¹æ•ˆæœã€‚

---

## ğŸ§ª æµ‹è¯•æ•°æ®é›†

æ ¹æ®æ‚¨æ‰“å¼€çš„æ–‡ä»¶ï¼Œæ‚¨æœ‰SECAPæ•°æ®é›†ï¼š

**æ•°æ®é›†**: `data/processed/secap/manifest.json`

**æµ‹è¯•å‘½ä»¤**ï¼š

```bash
# Edge Baseline - English
python experiments/runs/run_edge_baseline_cpu_limited.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --max_cpu_cores 2 \
    --max_memory_gb 16.0 \
    --output_name edge_cpu_limited_secap_en

# Cloud Baseline - English  
python experiments/runs/run_cloud_baseline.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --output_name cloud_secap_en
```

---

## ğŸ“ éªŒè¯æ¸…å•

ä¿®æ”¹åï¼Œæ£€æŸ¥ä»¥ä¸‹å„é¡¹ï¼š

### BLEUéªŒè¯
- [ ] ç»“æœä¸­æœ‰ `corpus_bleu_en` keyï¼ˆä¸æ˜¯`corpus_bleu_zh`ï¼‰
- [ ] BLEUåˆ†æ•°åˆç†ï¼ˆè‹±è¯­é€šå¸¸0.15-0.30ï¼‰
- [ ] æ—¥å¿—æ˜¾ç¤º"English tokenization"

### BERTScoreéªŒè¯
- [ ] ä½¿ç”¨`roberta-large`æ¨¡å‹ï¼ˆä¸æ˜¯`bert-base-chinese`ï¼‰
- [ ] åˆ†æ•°åˆç†ï¼ˆè‹±è¯­é€šå¸¸0.85-0.95ï¼‰

### ç”Ÿæˆè´¨é‡éªŒè¯
- [ ] è‹±è¯­å¥å­æµç•…ï¼Œè¯­æ³•æ­£ç¡®
- [ ] æ— æ ‡ç‚¹æ³›æ»¥ï¼ˆ", , , ,"ï¼‰
- [ ] 2-3å¥è¯ï¼ˆç¬¦åˆdetailed promptï¼‰
- [ ] æ— å¯¹è¯å¼å†…å®¹

### ä¸­æ–‡baselineéªŒè¯
- [ ] ä¸­æ–‡baselineç»§ç»­æ­£å¸¸å·¥ä½œ
- [ ] BLEUåˆ†æ•°ä¸ä¹‹å‰ä¸€è‡´
- [ ] æ— regression

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### BERTScoreçš„è¯­è¨€æ”¯æŒï¼ˆæ ¹æ®READMEï¼‰

| è¯­è¨€ | Model Type | è¯´æ˜ |
|------|-----------|------|
| **è‹±è¯­ (en)** | `roberta-large` | é»˜è®¤è‹±è¯­æ¨¡å‹ |
| **ä¸­æ–‡ (zh)** | `bert-base-chinese` | é»˜è®¤ä¸­æ–‡æ¨¡å‹ |
| **å…¶ä»–** | `bert-base-multilingual-cased` | å¤šè¯­è¨€æ¨¡å‹ |

**æˆ‘ä»¬çš„å®ç°**ï¼š
- âœ… ä¸­æ–‡ï¼š`hfl/chinese-roberta-wwm-ext-large`ï¼ˆæ›´å¥½çš„ä¸­æ–‡æ¨¡å‹ï¼‰
- âœ… è‹±æ–‡ï¼š`roberta-large`ï¼ˆæ ‡å‡†ï¼‰
- âœ… é€šè¿‡`language`å‚æ•°è‡ªåŠ¨é€‰æ‹©

---

### BLEUçš„Tokenizationï¼ˆæ ¹æ®sacreBLEUæ–‡æ¡£ï¼‰

| è¯­è¨€ | Tokenization | è¯´æ˜ |
|------|--------------|------|
| **ä¸­æ–‡** | `'zh'` | å­—ç¬¦çº§åˆ†è¯ |
| **è‹±è¯­** | `'13a'` | æ ‡å‡†è‹±è¯­ï¼ˆå¤„ç†æ ‡ç‚¹ã€å¤§å°å†™ï¼‰ |
| **å›½é™…** | `'intl'` | å›½é™…åŒ–tokenization |

**æˆ‘ä»¬éœ€è¦ä¿®æ”¹**ï¼š
- âŒ å½“å‰ï¼šæ‰€æœ‰è¯­è¨€éƒ½ç”¨`'zh'`
- âœ… ä¿®æ”¹åï¼šæ ¹æ®`language`å‚æ•°åŠ¨æ€é€‰æ‹©

---

## ğŸ¯ æ¨èå®æ–½æ–¹æ¡ˆ

### æœ€å°ä¿®æ”¹æ–¹æ¡ˆï¼ˆæ¨èï¼‰

**åªä¿®æ”¹BLEUè®¡ç®—**ï¼Œå…¶ä»–ä¿æŒä¸å˜ï¼š

1. âœ… **ä¿®æ”¹BLEU tokenization**ï¼ˆ3ä¸ªæ–‡ä»¶ï¼‰
   - æ ¹æ®languageå‚æ•°åŠ¨æ€é€‰æ‹©
   - ä¸­æ–‡ï¼š`'zh'`
   - è‹±æ–‡ï¼š`'13a'`

2. ğŸŸ¢ **ä¿æŒæ ‡ç‚¹æ§åˆ¶ä¸å˜**
   - æ ‡ç‚¹åˆ—è¡¨æš‚æ—¶ä¸æ”¹ï¼ˆå…ˆæµ‹è¯•ï¼‰
   - å¦‚æœè‹±è¯­å‡ºç°æ ‡ç‚¹é—®é¢˜ï¼Œå†æ‰©å±•åˆ—è¡¨

**åŸå› **ï¼š
- BERTScoreå·²ç»æ”¯æŒè‹±è¯­ âœ…
- Promptå·²ç»æ”¯æŒè‹±è¯­ âœ…
- åªæœ‰BLEUæ˜¯ç¡¬ç¼–ç çš„ âŒ
- æ ‡ç‚¹æ§åˆ¶å¯ä»¥åœ¨æµ‹è¯•åæŒ‰éœ€æ·»åŠ 

---

### å®Œæ•´ä¿®æ”¹æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰

å¦‚æœæµ‹è¯•å‘ç°è‹±è¯­æœ‰æ ‡ç‚¹é—®é¢˜ï¼Œå†æ·»åŠ ï¼š

1. âœ… æ‰©å±•æ ‡ç‚¹åˆ—è¡¨ï¼ˆåŒ…å«è‹±æ–‡æ ‡ç‚¹ï¼‰
2. âœ… æ”¹è¿›æ ‡ç‚¹é—¸é—¨ç»Ÿè®¡ï¼ˆç»Ÿè®¡æ‰€æœ‰tokenï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥

**éœ€è¦æˆ‘å¸®æ‚¨å®æ–½ä¿®æ”¹å—ï¼Ÿ**

æˆ‘å°†ï¼š
1. âœ… ä¿®æ”¹3ä¸ªbaselineè„šæœ¬çš„BLEUè®¡ç®—
2. âœ… ï¼ˆå¯é€‰ï¼‰æ‰©å±•Speculative Decodingçš„æ ‡ç‚¹åˆ—è¡¨
3. âœ… éªŒè¯ä»£ç æ— è¯­æ³•é”™è¯¯
4. âœ… æä¾›æµ‹è¯•å‘½ä»¤

**é¢„è®¡ä¿®æ”¹æ—¶é—´**: 10åˆ†é’Ÿ
**é¢„è®¡ä¿®æ”¹è¡Œæ•°**: 15-30è¡Œ

---

## ğŸ“ æ–‡æ¡£

å·²åˆ›å»ºï¼š
- `docs/chinese_specific_code_analysis.md` - ä¸­æ–‡ç‰¹å®šä»£ç åˆ†æ
- `docs/english_support_implementation_plan.md` - æœ¬æ–‡æ¡£

**å‡†å¤‡å¥½å¼€å§‹ä¿®æ”¹äº†å—ï¼Ÿ** ğŸš€

