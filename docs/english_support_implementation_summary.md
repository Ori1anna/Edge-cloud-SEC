# è‹±è¯­æ”¯æŒå®æ–½æ€»ç»“

## âœ… å®æ–½å®Œæˆæ—¶é—´
2025-10-12

---

## ğŸ“‹ å®Œæˆçš„ä¿®æ”¹

### Phase 1: BLEU Tokenizationï¼ˆğŸ”´ å¿…é¡»ï¼‰

#### ä¿®æ”¹1.1: `run_edge_baseline_cpu_limited.py`

**ä½ç½®**: ç¬¬531-541è¡Œ, ç¬¬571è¡Œ, ç¬¬590-591è¡Œ

**ä¿®æ”¹å†…å®¹**:
1. BLEUè®¡ç®—æ·»åŠ è¯­è¨€æ„ŸçŸ¥
2. ç»“æœkeyæ”¹ä¸ºåŠ¨æ€
3. æ—¥å¿—æ”¹ä¸ºåŠ¨æ€

**ä¿®æ”¹å‰**:
```python
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize='zh')  âŒ
"corpus_bleu_zh": overall_bleu  âŒ
logger.info(f"Corpus BLEU (Chinese tokenization): ...")  âŒ
```

**ä¿®æ”¹å**:
```python
bleu_tokenize = 'zh' if language == 'chinese' else '13a'  âœ…
corpus_bleu = sacrebleu.corpus_bleu(hyps, refs, tokenize=bleu_tokenize)  âœ…
f"corpus_bleu_{language[:2]}": overall_bleu  âœ… (corpus_bleu_zh æˆ– corpus_bleu_en)
lang_display = "Chinese" if language == 'chinese' else "English"  âœ…
logger.info(f"Corpus BLEU ({lang_display} tokenization): ...")  âœ…
```

---

#### ä¿®æ”¹1.2: `run_cloud_baseline.py`

**ä½ç½®**: ç¬¬261-271è¡Œ, ç¬¬295è¡Œ, ç¬¬314-315è¡Œ

**ä¿®æ”¹å†…å®¹**: ä¸Edge Baselineç›¸åŒ

---

#### ä¿®æ”¹1.3: `run_speculative_decoding_cpu_limited.py`

**ä½ç½®**: ç¬¬569-579è¡Œ, ç¬¬626è¡Œ, ç¬¬653-654è¡Œ

**ä¿®æ”¹å†…å®¹**: ä¸Edge Baselineç›¸åŒ

---

### Phase 2: æ ‡ç‚¹ç¬¦å·åˆ—è¡¨ï¼ˆğŸŸ¡ å»ºè®®ï¼‰

#### ä¿®æ”¹2.1: `speculative_decoding.py` - æ‰©å±•æ ‡ç‚¹åˆ—è¡¨

**ä½ç½®**: ç¬¬979-985è¡Œ, 1015-1026è¡Œ

**ä¿®æ”¹å‰**:
```python
PUNCT_IDS = _ids_for(['ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', ':', 'ï¼›', 'ï¼', 'ï¼Ÿ'])  âŒ åªæœ‰ä¸­æ–‡
COMMA_LIKE = _ids_for(['ï¼Œ', 'ã€', 'ï¼š', ':'])  âŒ
PERIOD_LIKE = _ids_for(['ã€‚'])  âŒ
```

**ä¿®æ”¹å**:
```python
PUNCT_IDS = _ids_for([
    # Chinese punctuation
    'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'ï¼›', 'ï¼', 'ï¼Ÿ',
    # English punctuation
    ',', '.', ';', ':', '!', '?'  âœ…
])

COMMA_LIKE = _ids_for([
    'ï¼Œ', 'ã€', 'ï¼š',  # Chinese
    ',', ';', ':'      # English  âœ…
])

PERIOD_LIKE = _ids_for([
    'ã€‚',  # Chinese
    '.'    # English  âœ…
])
```

---

### Phase 3: æ ‡ç‚¹é—¸é—¨ç»Ÿè®¡ï¼ˆğŸŸ¡ å»ºè®®ï¼‰

#### ä¿®æ”¹3.1: `speculative_decoding.py` - è¯­è¨€æ— å…³ç»Ÿè®¡

**ä½ç½®**: ç¬¬1028-1039è¡Œ

**ä¿®æ”¹å‰**:
```python
# åªç»Ÿè®¡CJKå­—ç¬¦
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    s = tokenizer.decode([t])
    if any('\u4e00' <= ch <= '\u9fff' for ch in s):  âŒ åªç»Ÿè®¡CJK
        since_punct += 1
```

**ä¿®æ”¹å**:
```python
# ç»Ÿè®¡æ‰€æœ‰éæ ‡ç‚¹tokenï¼ˆè¯­è¨€æ— å…³ï¼‰
since_punct = 0
for t in reversed(hist):
    if t in PUNCT_IDS:
        break
    # Count all non-punctuation tokens (works for both Chinese and English)
    since_punct += 1  âœ…
```

**é˜ˆå€¼è¯´æ˜**:
- é€—å·/å†’å·ï¼š4 tokens
  - ä¸­æ–‡ï¼š4ä¸ªå­—ç¬¦
  - è‹±æ–‡ï¼š2-4ä¸ªå•è¯ï¼ˆåˆç†ï¼‰
- å¥å·ï¼š5 tokens  
  - ä¸­æ–‡ï¼š5ä¸ªå­—ç¬¦
  - è‹±æ–‡ï¼š3-5ä¸ªå•è¯ï¼ˆåˆç†ï¼‰

**ç»“è®º**: é˜ˆå€¼å¯¹ä¸­è‹±æ–‡éƒ½é€‚ç”¨ï¼Œæ— éœ€è°ƒæ•´

---

#### ä¿®æ”¹3.2: æ›´æ–°æ³¨é‡Š

**ä½ç½®**: ç¬¬1041-1058è¡Œ

**ä¿®æ”¹å†…å®¹**: æ›´æ–°æ³¨é‡Šè¯´æ˜å¯¹ä¸­è‹±æ–‡éƒ½é€‚ç”¨

```python
# Comma/colon: require at least 4 content tokens
# Chinese: 4 tokens â‰ˆ 4 characters
# English: 4 tokens â‰ˆ 2-4 words (reasonable spacing)

# Period: require at least 5 content tokens
# Chinese: 5 tokens â‰ˆ 5 characters
# English: 5 tokens â‰ˆ 3-5 words (reasonable sentence length)
```

---

### Phase 4: è¯­æ³•é”™è¯¯ä¿®å¤

#### ä¿®å¤4.1: try-exceptç¼©è¿›é—®é¢˜

**ä½ç½®**: ç¬¬388-391è¡Œ

**é—®é¢˜**: elseå—å†…çš„ä»£ç ç¼©è¿›ä¸å¯¹

**ä¿®å¤**: è°ƒæ•´ç¼©è¿›ï¼Œç¡®ä¿`draft_tokens, draft_logits = ...`åœ¨elseå—å†…

---

#### ä¿®å¤4.2: if-elseç¼©è¿›é—®é¢˜

**ä½ç½®**: ç¬¬435-594è¡Œ

**é—®é¢˜**: elseå—å†…çš„ä»£ç ç¼©è¿›ä¸å¯¹

**ä¿®å¤**: è°ƒæ•´æ‰€æœ‰ç¼©è¿›ï¼Œç¡®ä¿æ­£ç¡®çš„åµŒå¥—ç»“æ„

---

#### ä¿®å¤4.3: Cloud verificationç¼©è¿›

**ä½ç½®**: ç¬¬507-594è¡Œ

**é—®é¢˜**: æ•´ä¸ªCloud verificationé€»è¾‘ç¼©è¿›ä¸å¯¹

**ä¿®å¤**: å°†æ‰€æœ‰Cloudç›¸å…³ä»£ç æ­£ç¡®ç¼©è¿›åˆ°elseå—å†…

---

#### ä¿®å¤4.4: é‡å¤çš„else

**ä½ç½®**: ç¬¬1270-1277è¡Œ

**é—®é¢˜**: ä¸¤ä¸ªelseå—é‡å¤

**ä¿®å¤**: åˆ é™¤ç¬¬äºŒä¸ªelseå—

---

## ğŸ“Š ä¿®æ”¹æ±‡æ€»

| æ–‡ä»¶ | ä¿®æ”¹ç±»å‹ | è¡Œæ•°å˜åŒ– | è¯´æ˜ |
|------|---------|---------|------|
| `run_edge_baseline_cpu_limited.py` | BLEU | +6 | è¯­è¨€æ„ŸçŸ¥BLEU |
| `run_cloud_baseline.py` | BLEU | +6 | è¯­è¨€æ„ŸçŸ¥BLEU |
| `run_speculative_decoding_cpu_limited.py` | BLEU | +6 | è¯­è¨€æ„ŸçŸ¥BLEU |
| `speculative_decoding.py` | æ ‡ç‚¹ | +10 | æ‰©å±•æ ‡ç‚¹åˆ—è¡¨ |
| `speculative_decoding.py` | é—¸é—¨ | +5 | è¯­è¨€æ— å…³ç»Ÿè®¡ |
| `speculative_decoding.py` | è¯­æ³• | ç¼©è¿›ä¿®å¤ | ä¿®å¤4å¤„ç¼©è¿›é”™è¯¯ |
| **æ€»è®¡** | - | **~33è¡Œ** | - |

---

## âœ… éªŒè¯ç»“æœ

### è¯­æ³•æ£€æŸ¥
- âœ… æ— è¯­æ³•é”™è¯¯
- âœ… åªæœ‰ç¯å¢ƒç›¸å…³çš„importè­¦å‘Šï¼ˆä¸å½±å“è¿è¡Œï¼‰

---

## ğŸ§ª æµ‹è¯•å‘½ä»¤

### è‹±è¯­Edge Baseline

```bash
cd /data/gpfs/projects/punim2341/jiajunlu/edgecloud-sec

python experiments/runs/run_edge_baseline_cpu_limited.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --max_cpu_cores 2 \
    --max_memory_gb 16.0 \
    --output_name edge_cpu_limited_secap_en
```

### è‹±è¯­Cloud Baseline

```bash
python experiments/runs/run_cloud_baseline.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --output_name cloud_secap_en
```

### è‹±è¯­Speculative Decoding

```bash
python experiments/runs/run_speculative_decoding_cpu_limited.py \
    --dataset_type unified \
    --dataset_path data/processed/secap/manifest.json \
    --caption_type original \
    --language english \
    --prompt_type detailed \
    --max_samples 10 \
    --output_name speculative_decoding_secap_en
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### BLEUç»“æœkey

| è¯­è¨€ | ç»“æœkey | Tokenization |
|------|---------|--------------|
| **ä¸­æ–‡** | `corpus_bleu_ch` | `'zh'` |
| **è‹±è¯­** | `corpus_bleu_en` | `'13a'` |

### æ—¥å¿—è¾“å‡º

**ä¸­æ–‡**:
```
INFO: Corpus BLEU (Chinese tokenization): 0.0250
```

**è‹±è¯­**:
```
INFO: Corpus BLEU (English tokenization): 0.2150
```

### BERTScoreæ¨¡å‹

| è¯­è¨€ | æ¨¡å‹ | è¯´æ˜ |
|------|------|------|
| **ä¸­æ–‡** | `hfl/chinese-roberta-wwm-ext-large` | ä¸­æ–‡ä¼˜åŒ–çš„RoBERTa |
| **è‹±è¯­** | `roberta-large` | æ ‡å‡†è‹±è¯­RoBERTa |

### æ ‡ç‚¹æ§åˆ¶

| è¯­è¨€ | æ ‡ç‚¹åˆ—è¡¨ | é—¸é—¨é˜ˆå€¼ | æ•ˆæœ |
|------|---------|----------|------|
| **ä¸­æ–‡** | ä¸­è‹±æ–‡æ ‡ç‚¹éƒ½æœ‰ | 4/5 tokens | âœ… é˜²æ­¢æ ‡ç‚¹æ³›æ»¥ |
| **è‹±è¯­** | ä¸­è‹±æ–‡æ ‡ç‚¹éƒ½æœ‰ | 4/5 tokens | âœ… é˜²æ­¢æ ‡ç‚¹æ³›æ»¥ |

---

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›

### 1. BLEUå‡†ç¡®æ€§

**ä¿®æ”¹å‰**:
- æ‰€æœ‰è¯­è¨€éƒ½ç”¨ä¸­æ–‡tokenization
- è‹±è¯­BLEUä¸å‡†ç¡®

**ä¿®æ”¹å**:
- ä¸­æ–‡ï¼šå­—ç¬¦çº§tokenization (`'zh'`)
- è‹±æ–‡ï¼šæ ‡å‡†tokenization (`'13a'`)
- BLEUåˆ†æ•°å‡†ç¡®

### 2. æ ‡ç‚¹æ§åˆ¶

**ä¿®æ”¹å‰**:
- åªæ§åˆ¶ä¸­æ–‡æ ‡ç‚¹
- è‹±è¯­å¯èƒ½å‡ºç°æ ‡ç‚¹æ³›æ»¥

**ä¿®æ”¹å**:
- æ§åˆ¶ä¸­è‹±æ–‡æ ‡ç‚¹
- ç»Ÿè®¡æ‰€æœ‰tokenï¼ˆä¸é™è¯­è¨€ï¼‰
- ä¸­è‹±æ–‡éƒ½èƒ½é˜²æ­¢æ ‡ç‚¹æ³›æ»¥

### 3. è¯­è¨€æ„ŸçŸ¥

**CJKç‰¹å®šçº¦æŸ**ï¼ˆä¿æŒä¸å˜ï¼‰:
- Repetition penalty (1.22)ï¼šåªå¯¹CJK
- Same-char blockingï¼šåªå¯¹CJK
- Content-only trigramï¼šåªå¯¹CJK

**æ•ˆæœ**:
- âœ… ä¸­æ–‡ï¼šåº”ç”¨æ‰€æœ‰çº¦æŸï¼ˆè§£å†³ç‰¹æœ‰é—®é¢˜ï¼‰
- âœ… è‹±æ–‡ï¼šåªåº”ç”¨æ ‡ç‚¹é—¸é—¨ï¼ˆè¶³å¤Ÿï¼‰

---

## ğŸ“ ç›¸å…³æ–‡æ¡£

1. `docs/chinese_specific_code_analysis.md` - ä¸­æ–‡ç‰¹å®šä»£ç åˆ†æ
2. `docs/english_support_implementation_plan.md` - è‹±è¯­æ”¯æŒæ–¹æ¡ˆ
3. `docs/english_support_implementation_summary.md` - æœ¬æ–‡æ¡£

---

## âœ… æ€»ç»“

### å®Œæˆçš„å·¥ä½œ

1. âœ… **BLEU Tokenization** - 3ä¸ªæ–‡ä»¶ï¼Œè¯­è¨€æ„ŸçŸ¥
2. âœ… **æ ‡ç‚¹ç¬¦å·åˆ—è¡¨** - æ‰©å±•åˆ°åŒ…å«è‹±æ–‡
3. âœ… **æ ‡ç‚¹é—¸é—¨ç»Ÿè®¡** - æ”¹ä¸ºè¯­è¨€æ— å…³
4. âœ… **è¯­æ³•é”™è¯¯ä¿®å¤** - 4å¤„ç¼©è¿›é”™è¯¯

### é¢„æœŸæ•ˆæœ

- âœ… ä¸­æ–‡baselineç»§ç»­æ­£å¸¸å·¥ä½œï¼ˆæ— regressionï¼‰
- âœ… è‹±è¯­baselineèƒ½å¤Ÿæ­£å¸¸è¿è¡Œ
- âœ… è‹±è¯­BLEUåˆ†æ•°å‡†ç¡®ï¼ˆä½¿ç”¨'13a' tokenizationï¼‰
- âœ… è‹±è¯­ç”Ÿæˆè´¨é‡åˆç†ï¼ˆæœ‰æ ‡ç‚¹æ§åˆ¶ï¼‰
- âœ… åªéœ€ä¿®æ”¹`--language english`å‚æ•°å³å¯åˆ‡æ¢

**æ‰€æœ‰ä¿®æ”¹å·²å®Œæˆï¼Œä»£ç æ— è¯­æ³•é”™è¯¯ï¼Œå¯ä»¥å¼€å§‹æµ‹è¯•è‹±è¯­baselineäº†ï¼** ğŸš€

