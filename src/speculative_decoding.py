"""
Speculative Decoding System Implementation
Coordinates between edge model (draft generation) and cloud model (verification)
"""

import torch
import time
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

from models.edge_model import EdgeModel
from models.cloud_model import CloudModel

logger = logging.getLogger(__name__)


@dataclass
class SpeculativeDecodingResult:
    """Result of speculative decoding process"""
    final_text: str
    accepted_tokens: List[int]
    rejected_tokens: List[int]
    verification_blocks: List[Dict]
    total_latency: float
    edge_latency: float
    cloud_latency: float
    acceptance_rate: float
    tokens_per_second: float
    latency_metrics: Dict[str, Any]


class SpeculativeDecodingSystem:
    """
    Complete speculative decoding system that coordinates edge and cloud models
    """
    
    def __init__(self, 
                 edge_model: EdgeModel,
                 cloud_model: CloudModel,
                 verification_threshold: float = 0.8,
                 max_verification_blocks: int = 3):
        """
        Initialize the speculative decoding system
        
        Args:
            edge_model: Edge model for draft generation
            cloud_model: Cloud model for verification
            verification_threshold: Threshold for block verification (0.0-1.0)
            max_verification_blocks: Maximum number of blocks to verify per round
        """
        self.edge_model = edge_model
        self.cloud_model = cloud_model
        self.verification_threshold = verification_threshold
        self.max_verification_blocks = max_verification_blocks
        
        logger.info(f"Initialized SpeculativeDecodingSystem with verification_threshold={verification_threshold}")
    
    def generate_with_speculative_decoding(self,
                                         audio_waveform: torch.Tensor,
                                         prompt: str = "Based on this audio, describe the emotional state of the speaker in Chinese.",
                                         block_size: int = 4,
                                         max_blocks: int = 8,
                                         temperature: float = 0.7,
                                         top_p: float = 0.9) -> SpeculativeDecodingResult:
        """
        Main speculative decoding pipeline
        
        Args:
            audio_waveform: Audio input tensor
            prompt: Text prompt for generation
            block_size: Number of tokens per block
            max_blocks: Maximum number of blocks to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            
        Returns:
            SpeculativeDecodingResult with complete generation details
        """
        total_start_time = time.time()
        
        logger.info("Starting speculative decoding process...")
        logger.info(f"Parameters: block_size={block_size}, max_blocks={max_blocks}, temperature={temperature}")
        
        # Step 1: Generate draft blocks using edge model
        logger.info("Step 1: Generating draft blocks with edge model...")
        edge_start_time = time.time()
        
        try:
            blocks_with_uncertainty, edge_latency_metrics = self.edge_model.generate_draft_blocks(
                audio_features=audio_waveform,
                prompt=prompt,
                block_size=block_size,
                max_blocks=max_blocks,
                temperature=temperature,
                top_p=top_p
            )
            edge_latency = time.time() - edge_start_time
            
            logger.info(f"Generated {len(blocks_with_uncertainty)} draft blocks in {edge_latency:.3f}s")
            
        except Exception as e:
            logger.error(f"Edge model generation failed: {e}")
            return self._create_error_result(str(e), total_start_time)
        
        if not blocks_with_uncertainty:
            logger.warning("No blocks generated by edge model")
            return self._create_error_result("No blocks generated", total_start_time)
        
        # Step 2: Identify blocks that need verification
        verification_candidates = self._select_verification_candidates(blocks_with_uncertainty)
        logger.info(f"Selected {len(verification_candidates)} blocks for verification")
        
        # Step 3: Verify selected blocks with cloud model
        cloud_start_time = time.time()
        verification_results = []
        
        if verification_candidates:
            logger.info("Step 2: Verifying blocks with cloud model...")
            verification_results = self._verify_blocks_with_cloud(
                verification_candidates, audio_waveform, prompt, temperature, top_p
            )
        
        cloud_latency = time.time() - cloud_start_time
        
        # Step 4: Accept/reject tokens based on verification
        logger.info("Step 3: Accepting/rejecting tokens based on verification...")
        acceptance_result = self._accept_reject_tokens(
            blocks_with_uncertainty, verification_results
        )
        
        # Step 5: Generate final text
        final_text = self._generate_final_text(acceptance_result['accepted_tokens'])
        
        total_latency = time.time() - total_start_time
        
        # Calculate metrics
        total_tokens = len(acceptance_result['accepted_tokens']) + len(acceptance_result['rejected_tokens'])
        acceptance_rate = len(acceptance_result['accepted_tokens']) / max(total_tokens, 1)
        tokens_per_second = len(acceptance_result['accepted_tokens']) / max(total_latency, 0.001)
        
        logger.info(f"Speculative decoding completed:")
        logger.info(f"  Total latency: {total_latency:.3f}s")
        logger.info(f"  Edge latency: {edge_latency:.3f}s")
        logger.info(f"  Cloud latency: {cloud_latency:.3f}s")
        logger.info(f"  Acceptance rate: {acceptance_rate:.2%}")
        logger.info(f"  Tokens per second: {tokens_per_second:.2f}")
        
        return SpeculativeDecodingResult(
            final_text=final_text,
            accepted_tokens=acceptance_result['accepted_tokens'],
            rejected_tokens=acceptance_result['rejected_tokens'],
            verification_blocks=verification_results,
            total_latency=total_latency,
            edge_latency=edge_latency,
            cloud_latency=cloud_latency,
            acceptance_rate=acceptance_rate,
            tokens_per_second=tokens_per_second,
            latency_metrics={
                'edge_metrics': edge_latency_metrics,
                'total_tokens': total_tokens,
                'accepted_tokens': len(acceptance_result['accepted_tokens']),
                'rejected_tokens': len(acceptance_result['rejected_tokens'])
            }
        )
    
    def _select_verification_candidates(self, blocks_with_uncertainty: List[Dict]) -> List[Dict]:
        """
        Select blocks that need verification based on uncertainty and content patterns
        
        Args:
            blocks_with_uncertainty: List of blocks with uncertainty information
            
        Returns:
            List of blocks selected for verification
        """
        verification_candidates = []
        
        for block in blocks_with_uncertainty:
            # Check if block should be verified based on uncertainty signals
            should_verify = block.get('should_verify', False)
            
            if should_verify:
                verification_candidates.append(block)
                
                # Limit number of verification candidates
                if len(verification_candidates) >= self.max_verification_blocks:
                    break
        
        logger.info(f"Selected {len(verification_candidates)} blocks for verification out of {len(blocks_with_uncertainty)} total blocks")
        
        return verification_candidates
    
    def _verify_blocks_with_cloud(self, 
                                verification_candidates: List[Dict],
                                audio_waveform: torch.Tensor,
                                prompt: str,
                                temperature: float,
                                top_p: float) -> List[Dict]:
        """
        Verify selected blocks using cloud model
        
        Args:
            verification_candidates: Blocks to verify
            audio_waveform: Original audio input
            prompt: Original prompt
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            
        Returns:
            List of verification results
        """
        verification_results = []
        
        for i, block in enumerate(verification_candidates):
            try:
                logger.info(f"Verifying block {i+1}/{len(verification_candidates)}")
                
                # Generate verification text using cloud model
                verification_text, cloud_metrics = self.cloud_model.generate_independently(
                    audio_waveform=audio_waveform,
                    prompt=prompt,
                    max_new_tokens=len(block['tokens']) + 4,  # Generate slightly more tokens
                    temperature=temperature,
                    top_p=top_p
                )
                
                # Tokenize verification text
                verification_tokens = self.cloud_model.processor.tokenizer.encode(
                    verification_text, add_special_tokens=False
                )
                
                verification_result = {
                    'block_idx': block['block_idx'],
                    'original_tokens': block['tokens'],
                    'original_text': block['text'],
                    'verification_tokens': verification_tokens,
                    'verification_text': verification_text,
                    'cloud_metrics': cloud_metrics,
                    'verification_time': time.time()
                }
                
                verification_results.append(verification_result)
                
                logger.info(f"Block {i+1} verification completed: {len(verification_tokens)} tokens generated")
                
            except Exception as e:
                logger.error(f"Failed to verify block {i+1}: {e}")
                # Add empty verification result to maintain alignment
                verification_results.append({
                    'block_idx': block['block_idx'],
                    'original_tokens': block['tokens'],
                    'original_text': block['text'],
                    'verification_tokens': [],
                    'verification_text': '',
                    'cloud_metrics': {},
                    'verification_time': time.time(),
                    'error': str(e)
                })
        
        return verification_results
    
    def _accept_reject_tokens(self, 
                            blocks_with_uncertainty: List[Dict],
                            verification_results: List[Dict]) -> Dict[str, List[int]]:
        """
        Accept or reject tokens based on verification results
        
        Args:
            blocks_with_uncertainty: Original draft blocks
            verification_results: Cloud model verification results
            
        Returns:
            Dictionary with accepted and rejected tokens
        """
        accepted_tokens = []
        rejected_tokens = []
        
        # Create a mapping of block_idx to verification results
        verification_map = {vr['block_idx']: vr for vr in verification_results}
        
        for block in blocks_with_uncertainty:
            block_idx = block['block_idx']
            original_tokens = block['tokens']
            
            if block_idx in verification_map:
                # This block was verified - compare tokens
                verification_result = verification_map[block_idx]
                verification_tokens = verification_result.get('verification_tokens', [])
                
                # Simple token-by-token comparison
                min_length = min(len(original_tokens), len(verification_tokens))
                
                for i in range(min_length):
                    if original_tokens[i] == verification_tokens[i]:
                        accepted_tokens.append(original_tokens[i])
                    else:
                        rejected_tokens.append(original_tokens[i])
                        # Accept the verification token instead
                        accepted_tokens.append(verification_tokens[i])
                
                # Handle remaining tokens
                if len(original_tokens) > min_length:
                    # Original block was longer - reject remaining tokens
                    rejected_tokens.extend(original_tokens[min_length:])
                elif len(verification_tokens) > min_length:
                    # Verification was longer - accept additional tokens
                    accepted_tokens.extend(verification_tokens[min_length:])
                
                logger.info(f"Block {block_idx}: {min_length} tokens compared, {len(accepted_tokens)} total accepted")
                
            else:
                # This block was not verified - accept all tokens
                accepted_tokens.extend(original_tokens)
                logger.info(f"Block {block_idx}: Not verified, accepting all {len(original_tokens)} tokens")
        
        return {
            'accepted_tokens': accepted_tokens,
            'rejected_tokens': rejected_tokens
        }
    
    def _generate_final_text(self, accepted_tokens: List[int]) -> str:
        """
        Generate final text from accepted tokens
        
        Args:
            accepted_tokens: List of accepted token IDs
            
        Returns:
            Final generated text
        """
        if not accepted_tokens:
            return ""
        
        try:
            # Use edge model's tokenizer to decode
            final_text = self.edge_model.processor.tokenizer.decode(
                accepted_tokens, skip_special_tokens=True
            )
            return final_text.strip()
        except Exception as e:
            logger.error(f"Failed to decode final text: {e}")
            return ""
    
    def _create_error_result(self, error_message: str, start_time: float) -> SpeculativeDecodingResult:
        """
        Create error result when generation fails
        
        Args:
            error_message: Error description
            start_time: Start time for latency calculation
            
        Returns:
            SpeculativeDecodingResult with error information
        """
        total_latency = time.time() - start_time
        
        return SpeculativeDecodingResult(
            final_text=f"Error: {error_message}",
            accepted_tokens=[],
            rejected_tokens=[],
            verification_blocks=[],
            total_latency=total_latency,
            edge_latency=0.0,
            cloud_latency=0.0,
            acceptance_rate=0.0,
            tokens_per_second=0.0,
            latency_metrics={'error': error_message}
        )
