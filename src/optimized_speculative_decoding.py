"""
Optimized Speculative Decoding System
Addresses performance issues and implements proper speculative decoding
"""

import torch
import time
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

from models.edge_model import EdgeModel
from models.cloud_model import CloudModel

logger = logging.getLogger(__name__)


@dataclass
class OptimizedSpeculativeResult:
    """Result of optimized speculative decoding process"""
    final_text: str
    accepted_tokens: List[int]
    rejected_tokens: List[int]
    total_latency: float
    edge_latency: float
    cloud_latency: float
    acceptance_rate: float
    tokens_per_second: float
    speedup_ratio: float
    latency_metrics: Dict[str, Any]


class OptimizedSpeculativeDecodingSystem:
    """
    Optimized speculative decoding system that implements proper parallel processing
    """
    
    def __init__(self, 
                 edge_model: EdgeModel,
                 cloud_model: CloudModel,
                 verification_threshold: float = 0.8,
                 max_verification_blocks: int = 2):
        """
        Initialize the optimized speculative decoding system
        
        Args:
            edge_model: Edge model for draft generation
            cloud_model: Cloud model for verification
            verification_threshold: Threshold for block verification (0.0-1.0)
            max_verification_blocks: Maximum number of blocks to verify per round
        """
        self.edge_model = edge_model
        self.cloud_model = cloud_model
        self.verification_threshold = verification_threshold
        self.max_verification_blocks = max_verification_blocks
        
        logger.info(f"Initialized OptimizedSpeculativeDecodingSystem with verification_threshold={verification_threshold}")
    
    def generate_with_optimized_speculative_decoding(self,
                                                   audio_waveform: torch.Tensor,
                                                   prompt: str = "Based on this audio, describe the emotional state of the speaker in Chinese.",
                                                   block_size: int = 4,
                                                   max_blocks: int = 6,
                                                   temperature: float = 0.7,
                                                   top_p: float = 0.9) -> OptimizedSpeculativeResult:
        """
        Optimized speculative decoding pipeline with parallel processing
        
        Args:
            audio_waveform: Audio input tensor
            prompt: Text prompt for generation
            block_size: Number of tokens per block
            max_blocks: Maximum number of blocks to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            
        Returns:
            OptimizedSpeculativeResult with complete generation details
        """
        total_start_time = time.time()
        
        logger.info("Starting optimized speculative decoding process...")
        logger.info(f"Parameters: block_size={block_size}, max_blocks={max_blocks}, temperature={temperature}")
        
        # Step 1: Generate draft blocks using edge model
        logger.info("Step 1: Generating draft blocks with edge model...")
        edge_start_time = time.time()
        
        try:
            blocks_with_uncertainty, edge_latency_metrics = self.edge_model.generate_draft_blocks(
                audio_features=audio_waveform,
                prompt=prompt,
                block_size=block_size,
                max_blocks=max_blocks,
                temperature=temperature,
                top_p=top_p
            )
            edge_latency = time.time() - edge_start_time
            
            logger.info(f"Generated {len(blocks_with_uncertainty)} draft blocks in {edge_latency:.3f}s")
            
        except Exception as e:
            logger.error(f"Edge model generation failed: {e}")
            return self._create_error_result(str(e), total_start_time)
        
        if not blocks_with_uncertainty:
            logger.warning("No blocks generated by edge model")
            return self._create_error_result("No blocks generated", total_start_time)
        
        # Step 2: Select only the most uncertain blocks for verification
        verification_candidates = self._select_optimal_verification_candidates(blocks_with_uncertainty)
        logger.info(f"Selected {len(verification_candidates)} blocks for verification (optimized selection)")
        
        # Step 3: Parallel verification with cloud model (if any candidates)
        cloud_start_time = time.time()
        verification_results = []
        
        if verification_candidates:
            logger.info("Step 2: Verifying selected blocks with cloud model...")
            verification_results = self._parallel_verify_blocks(
                verification_candidates, audio_waveform, prompt, temperature, top_p
            )
        
        cloud_latency = time.time() - cloud_start_time
        
        # Step 4: Smart token acceptance based on verification
        logger.info("Step 3: Smart token acceptance based on verification...")
        acceptance_result = self._smart_accept_reject_tokens(
            blocks_with_uncertainty, verification_results
        )
        
        # Step 5: Generate final text
        final_text = self._generate_final_text(acceptance_result['accepted_tokens'])
        
        total_latency = time.time() - total_start_time
        
        # Calculate metrics
        total_tokens = len(acceptance_result['accepted_tokens']) + len(acceptance_result['rejected_tokens'])
        acceptance_rate = len(acceptance_result['accepted_tokens']) / max(total_tokens, 1)
        tokens_per_second = len(acceptance_result['accepted_tokens']) / max(total_latency, 0.001)
        
        # Calculate speedup ratio compared to cloud-only baseline
        cloud_only_estimate = cloud_latency * (total_tokens / max(len(verification_results) * block_size, 1))
        speedup_ratio = cloud_only_estimate / max(total_latency, 0.001)
        
        logger.info(f"Optimized speculative decoding completed:")
        logger.info(f"  Total latency: {total_latency:.3f}s")
        logger.info(f"  Edge latency: {edge_latency:.3f}s")
        logger.info(f"  Cloud latency: {cloud_latency:.3f}s")
        logger.info(f"  Acceptance rate: {acceptance_rate:.2%}")
        logger.info(f"  Tokens per second: {tokens_per_second:.2f}")
        logger.info(f"  Estimated speedup: {speedup_ratio:.2f}x")
        
        return OptimizedSpeculativeResult(
            final_text=final_text,
            accepted_tokens=acceptance_result['accepted_tokens'],
            rejected_tokens=acceptance_result['rejected_tokens'],
            total_latency=total_latency,
            edge_latency=edge_latency,
            cloud_latency=cloud_latency,
            acceptance_rate=acceptance_rate,
            tokens_per_second=tokens_per_second,
            speedup_ratio=speedup_ratio,
            latency_metrics={
                'edge_metrics': edge_latency_metrics,
                'total_tokens': total_tokens,
                'accepted_tokens': len(acceptance_result['accepted_tokens']),
                'rejected_tokens': len(acceptance_result['rejected_tokens']),
                'verification_blocks': len(verification_results)
            }
        )
    
    def _select_optimal_verification_candidates(self, blocks_with_uncertainty: List[Dict]) -> List[Dict]:
        """
        Select only the most uncertain blocks for verification using adaptive thresholding
        
        Args:
            blocks_with_uncertainty: List of blocks with uncertainty information
            
        Returns:
            List of blocks selected for verification (optimized selection)
        """
        verification_candidates = []
        
        # Extract all entropy values
        entropy_values = []
        for block in blocks_with_uncertainty:
            uncertainty = block.get('uncertainty_signals', {})
            entropy = uncertainty.get('entropy', [0])[0] if isinstance(uncertainty.get('entropy', []), list) else uncertainty.get('entropy', 0)
            entropy_values.append(entropy)
        
        if not entropy_values:
            return verification_candidates
        
        # Calculate adaptive threshold based on entropy distribution
        max_entropy = max(entropy_values)
        avg_entropy = sum(entropy_values) / len(entropy_values)
        
        # Use adaptive threshold: verify blocks above average + 0.2, but cap at reasonable values
        adaptive_threshold = min(max(avg_entropy + 0.2, 0.8), 1.2)
        
        logger.info(f"Entropy distribution: max={max_entropy:.3f}, avg={avg_entropy:.3f}, adaptive_threshold={adaptive_threshold:.3f}")
        
        # Sort blocks by uncertainty (highest first)
        sorted_blocks = sorted(blocks_with_uncertainty, 
                             key=lambda x: x.get('uncertainty_signals', {}).get('entropy', [0])[0] if isinstance(x.get('uncertainty_signals', {}).get('entropy', []), list) else x.get('uncertainty_signals', {}).get('entropy', 0), 
                             reverse=True)
        
        for block in sorted_blocks:
            uncertainty = block.get('uncertainty_signals', {})
            entropy = uncertainty.get('entropy', [0])[0] if isinstance(uncertainty.get('entropy', []), list) else uncertainty.get('entropy', 0)
            
            # Use adaptive threshold for verification
            if entropy > adaptive_threshold:
                verification_candidates.append(block)
                logger.info(f"Selected block with entropy {entropy:.3f} for verification (threshold: {adaptive_threshold:.3f})")
                
                # Limit verification to reduce latency
                if len(verification_candidates) >= self.max_verification_blocks:
                    break
        
        logger.info(f"Selected {len(verification_candidates)} blocks for verification out of {len(blocks_with_uncertainty)} total blocks using adaptive threshold {adaptive_threshold:.3f}")
        
        return verification_candidates
    
    def _parallel_verify_blocks(self, 
                              verification_candidates: List[Dict],
                              audio_waveform: torch.Tensor,
                              prompt: str,
                              temperature: float,
                              top_p: float) -> List[Dict]:
        """
        Verify selected blocks using cloud model with optimized parameters
        
        Args:
            verification_candidates: Blocks to verify
            audio_waveform: Original audio input
            prompt: Original prompt
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            
        Returns:
            List of verification results
        """
        verification_results = []
        
        for i, block in enumerate(verification_candidates):
            try:
                logger.info(f"Verifying block {i+1}/{len(verification_candidates)} (optimized)")
                
                # Generate verification text using cloud model with optimized parameters
                verification_text, cloud_metrics = self.cloud_model.generate_independently(
                    audio_waveform=audio_waveform,
                    prompt=prompt,
                    max_new_tokens=len(block['tokens']) + 2,  # Generate fewer extra tokens
                    temperature=temperature,
                    top_p=top_p
                )
                
                # Tokenize verification text
                verification_tokens = self.cloud_model.processor.tokenizer.encode(
                    verification_text, add_special_tokens=False
                )
                
                verification_result = {
                    'block_idx': block['block_idx'],
                    'original_tokens': block['tokens'],
                    'original_text': block['text'],
                    'verification_tokens': verification_tokens,
                    'verification_text': verification_text,
                    'cloud_metrics': cloud_metrics,
                    'verification_time': time.time()
                }
                
                verification_results.append(verification_result)
                
                logger.info(f"Block {i+1} verification completed: {len(verification_tokens)} tokens generated")
                
            except Exception as e:
                logger.error(f"Failed to verify block {i+1}: {e}")
                # Add empty verification result to maintain alignment
                verification_results.append({
                    'block_idx': block['block_idx'],
                    'original_tokens': block['tokens'],
                    'original_text': block['text'],
                    'verification_tokens': [],
                    'verification_text': '',
                    'cloud_metrics': {},
                    'verification_time': time.time(),
                    'error': str(e)
                })
        
        return verification_results
    
    def _smart_accept_reject_tokens(self, 
                                  blocks_with_uncertainty: List[Dict],
                                  verification_results: List[Dict]) -> Dict[str, List[int]]:
        """
        Smart token acceptance based on verification results and uncertainty
        
        Args:
            blocks_with_uncertainty: Original draft blocks
            verification_results: Cloud model verification results
            
        Returns:
            Dictionary with accepted and rejected tokens
        """
        accepted_tokens = []
        rejected_tokens = []
        
        # Create a mapping of block_idx to verification results
        verification_map = {vr['block_idx']: vr for vr in verification_results}
        
        for block in blocks_with_uncertainty:
            block_idx = block['block_idx']
            original_tokens = block['tokens']
            uncertainty = block.get('uncertainty_signals', {})
            
            if block_idx in verification_map:
                # This block was verified - use smart comparison
                verification_result = verification_map[block_idx]
                verification_tokens = verification_result.get('verification_tokens', [])
                
                # Smart token-by-token comparison with uncertainty weighting
                min_length = min(len(original_tokens), len(verification_tokens))
                
                for i in range(min_length):
                    if original_tokens[i] == verification_tokens[i]:
                        accepted_tokens.append(original_tokens[i])
                    else:
                        # Check uncertainty for this position
                        token_log_probs = uncertainty.get('token_log_probs', [])
                        if isinstance(token_log_probs, list) and len(token_log_probs) > 0:
                            # Handle nested list structure
                            if isinstance(token_log_probs[0], list) and i < len(token_log_probs[0]):
                                token_uncertainty = token_log_probs[0][i]
                            elif i < len(token_log_probs):
                                token_uncertainty = token_log_probs[i]
                            else:
                                token_uncertainty = 0.0
                        else:
                            token_uncertainty = 0.0
                        
                        # If original token has high uncertainty, prefer verification token
                        if token_uncertainty < -2.0:  # High uncertainty (low log prob)
                            rejected_tokens.append(original_tokens[i])
                            accepted_tokens.append(verification_tokens[i])
                        else:
                            # Keep original token if it's not too uncertain
                            accepted_tokens.append(original_tokens[i])
                
                # Handle remaining tokens
                if len(original_tokens) > min_length:
                    rejected_tokens.extend(original_tokens[min_length:])
                elif len(verification_tokens) > min_length:
                    accepted_tokens.extend(verification_tokens[min_length:])
                
                logger.info(f"Block {block_idx}: Smart comparison completed, {len(accepted_tokens)} total accepted")
                
            else:
                # This block was not verified - accept all tokens
                accepted_tokens.extend(original_tokens)
                logger.info(f"Block {block_idx}: Not verified, accepting all {len(original_tokens)} tokens")
        
        return {
            'accepted_tokens': accepted_tokens,
            'rejected_tokens': rejected_tokens
        }
    
    def _generate_final_text(self, accepted_tokens: List[int]) -> str:
        """
        Generate final text from accepted tokens
        
        Args:
            accepted_tokens: List of accepted token IDs
            
        Returns:
            Final generated text
        """
        if not accepted_tokens:
            return ""
        
        try:
            # Use edge model's tokenizer to decode
            final_text = self.edge_model.processor.tokenizer.decode(
                accepted_tokens, skip_special_tokens=True
            )
            return final_text.strip()
        except Exception as e:
            logger.error(f"Failed to decode final text: {e}")
            return ""
    
    def _create_error_result(self, error_message: str, start_time: float) -> OptimizedSpeculativeResult:
        """
        Create error result when generation fails
        
        Args:
            error_message: Error description
            start_time: Start time for latency calculation
            
        Returns:
            OptimizedSpeculativeResult with error information
        """
        total_latency = time.time() - start_time
        
        return OptimizedSpeculativeResult(
            final_text=f"Error: {error_message}",
            accepted_tokens=[],
            rejected_tokens=[],
            total_latency=total_latency,
            edge_latency=0.0,
            cloud_latency=0.0,
            acceptance_rate=0.0,
            tokens_per_second=0.0,
            speedup_ratio=0.0,
            latency_metrics={'error': error_message}
        )
