/data/gpfs/projects/punim2341/jiajunlu/conda/envs/sec-gpu/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[nltk_data] Downloading package wordnet to /home/jiajunlu/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/jiajunlu/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
INFO:__main__:Loading unified manifest from data/processed/mer2024/manifest_audio_text_augmented_v5.json
INFO:__main__:Loaded 332 samples from unified manifest
INFO:__main__:Processing 332 samples with CPU-limited edge + GPU cloud speculative decoding
INFO:__main__:Caption type: audio_only
INFO:__main__:Language: english
INFO:__main__:Prompt type: detailed
INFO:__main__:Entropy threshold: 3.5
INFO:__main__:K (draft tokens): 5
INFO:__main__:Edge hardware limits: 2 cores, 16.0GB memory
INFO:__main__:Cloud device: GPU (G100)
INFO:__main__:Hardware Limiter initialized:
INFO:__main__:  Original CPU cores: 64
INFO:__main__:  Original memory: 1006.9GB
INFO:__main__:  Limited CPU cores: 2
INFO:__main__:  Limited memory: 16.0GB
INFO:__main__:Edge device: cpu
INFO:__main__:Cloud device: cuda
INFO:__main__:Initializing LimitedEdgeModel with device=cpu, dtype=float32
INFO:src.models.edge_model:Initializing EdgeModel with model_name=Qwen/Qwen2.5-Omni-3B, device=cpu, dtype=float32
INFO:src.models.edge_model:Loading edge model: Qwen/Qwen2.5-Omni-3B
INFO:src.models.edge_model:This may take several minutes for the first time...
INFO:src.models.edge_model:Loading processor...
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
INFO:src.models.edge_model:Processor loaded successfully
INFO:src.models.edge_model:Loading model (this may take a while)...
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]
INFO:src.models.edge_model:Edge model loaded successfully
INFO:src.models.edge_model:EdgeModel initialization completed successfully
INFO:__main__:Applied CPU optimizations with 2 threads
INFO:src.models.cloud_model:Loading cloud model: Qwen/Qwen2.5-Omni-7B
INFO:src.models.cloud_model:This may take several minutes for the first time...
INFO:src.models.cloud_model:Loading processor...
INFO:src.models.cloud_model:Processor loaded successfully
INFO:src.models.cloud_model:Loading model (this may take a while)...
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:06<00:24,  6.22s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:11<00:16,  5.41s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:15<00:10,  5.18s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:20<00:05,  5.08s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.57s/it]
INFO:src.models.cloud_model:Cloud model loaded successfully
INFO:src.speculative_decoding:Initialized SimpleSpeculativeDecoding with entropy_threshold=3.5, k=5, target_sentences=2, language=chinese, prompt_type=detailed
INFO:__main__:Using prompt template: As an expert in the field of emotions, please focus on the acoustic information in the audio to disc...
INFO:__main__:Processing sample 1/332: sample_00000000
INFO:src.speculative_decoding:Speculative Decoding prompt: ['<|im_start|>system\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.<|im_end|>\n<|im_start|>user\n<|audio_bos|><|AUDIO|><|audio_eos|>As an expert in the field of emotions, please focus on the acoustic information in the audio to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual.<|im_end|>\n<|im_start|>assistant\n']
INFO:src.speculative_decoding:Input keys: ['input_ids', 'attention_mask', 'feature_attention_mask', 'input_features']
INFO:src.speculative_decoding:Audio features shape: torch.Size([1, 128, 30000])
INFO:src.speculative_decoding:Starting speculative decoding with max_new_tokens=128
INFO:src.speculative_decoding:Performing multimodal prefill for efficient incremental generation...
INFO:src.speculative_decoding:Performing multimodal prefill with Edge model...
INFO:src.speculative_decoding:Edge prefill completed in 3.928s
INFO:src.speculative_decoding:Multimodal prefill completed in 3.931s
INFO:src.speculative_decoding:Added stopping criteria for chinese with sentence_end_chars=('。', '.'), min_chars=90, min_new_tokens=48, prompt_type=detailed
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=0/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: 'The'
INFO:src.speculative_decoding:Edge generated 5 tokens: [785, 1697, 4977, 5008, 35118]
INFO:src.speculative_decoding:Token uncertainties: [3.4791622161865234, 2.2164700031280518, 2.1409518718719482, 1.284088134765625, 4.853043079376221]
INFO:src.speculative_decoding:High uncertainty (4.853 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.0793', '0.2208', '0.2325', '0.1178', '0.0384']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=5, check_sequence_len=185
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=5/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [13, 10964, 16232, 374, 14400]
INFO:src.speculative_decoding:Token uncertainties: [1.6895653009414673, 2.4490232467651367, 1.9185459613800049, 1.2733855247497559, 3.1391210556030273]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: . Their tone is somewhat...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.1796', '0.1779', '0.1914', '0.7415', '0.1234']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=10, check_sequence_len=190
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=10/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [70460, 979, 807, 1977, 330]
INFO:src.speculative_decoding:Token uncertainties: [2.5444231033325195, 1.2721381187438965, 1.5076768398284912, 0.4705263078212738, 1.354881763458252]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  hesitant when they say "...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.7596', '0.0068', '0.6711', '0.9582', '0.9607']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=15, check_sequence_len=195
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=15/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [35946, 99459, 99392, 103924, 1]
INFO:src.speculative_decoding:Token uncertainties: [0.7806749939918518, 0.6323063969612122, 0.041460681706666946, 0.2510654628276825, 1.1537630558013916]
INFO:src.speculative_decoding:Low uncertainty (1.154 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=20, check_sequence_len=200
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=20/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [3837, 8206, 46918, 17267, 438]
INFO:src.speculative_decoding:Token uncertainties: [1.4140037298202515, 2.8806328773498535, 2.447845697402954, 0.3541516959667206, 0.5110021233558655]
INFO:src.speculative_decoding:Low uncertainty (2.881 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=25, check_sequence_len=205
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=25/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [364, 40, 1513, 944, 1414]
INFO:src.speculative_decoding:Token uncertainties: [0.37021827697753906, 0.22417058050632477, 0.24467253684997559, 0.021625783294439316, 0.1421053111553192]
INFO:src.speculative_decoding:Low uncertainty (0.370 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=30, check_sequence_len=210
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=30/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [4427, 1096, 13230, 429, 1052]
INFO:src.speculative_decoding:Token uncertainties: [2.474355936050415, 2.338731288909912, 2.7281949520111084, 1.1627118587493896, 0.6118156909942627]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: '. This suggests that there...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.1964', '0.1252', '0.3898', '0.0086', '0.0168']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=35, check_sequence_len=215
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=35/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' or'
INFO:src.speculative_decoding:Edge generated 5 tokens: [594, 1045, 10492, 476, 26826]
INFO:src.speculative_decoding:Token uncertainties: [1.3016074895858765, 0.5829368233680725, 2.7685203552246094, 0.7403615713119507, 2.490870475769043]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: 's some doubt or uncertainty...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.7079', '0.0997', '0.4414', '0.9459', '0.0925']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=40, check_sequence_len=220
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=40/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' in'
INFO:src.speculative_decoding:Edge generated 5 tokens: [304, 862, 3971, 13, 2379]
INFO:src.speculative_decoding:Token uncertainties: [1.5913059711456299, 0.6213725805282593, 2.1344120502471924, 0.6747484803199768, 3.254767417907715]
INFO:src.speculative_decoding:Low uncertainty (3.255 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=45, check_sequence_len=225
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=45/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [1083, 6286, 537, 3432, 3139]
INFO:src.speculative_decoding:Token uncertainties: [1.0201441049575806, 1.9785486459732056, 1.8556777238845825, 0.017237570136785507, 1.969411849975586]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  also mention not having experience...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.7625', '0.5935', '0.4141', '0.9977', '0.2159']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=50, check_sequence_len=230
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=50/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [11, 892, 4623, 14807, 264]
INFO:src.speculative_decoding:Token uncertainties: [0.8580759763717651, 2.364384174346924, 2.18501877784729, 2.2736055850982666, 1.8705538511276245]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: , which further indicates a...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.1030', '0.8000', '0.4268', '0.4315', '0.6248']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=55, check_sequence_len=235
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=55/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' of'
INFO:src.speculative_decoding:Edge generated 5 tokens: [6853, 315, 12136, 13, 27893]
INFO:src.speculative_decoding:Token uncertainties: [0.44304439425468445, 0.0015918860444799066, 1.2748464345932007, 1.2114511728286743, 2.1567490100860596]
INFO:src.speculative_decoding:Low uncertainty (2.157 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=60, check_sequence_len=240
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=60/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' the'
INFO:src.speculative_decoding:Edge generated 5 tokens: [358, 4172, 7023, 279, 3842]
INFO:src.speculative_decoding:Token uncertainties: [0.016638481989502907, 0.8888506889343262, 0.5686829090118408, 1.1492279767990112, 1.1949013471603394]
INFO:src.speculative_decoding:Low uncertainty (1.195 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=65, check_sequence_len=245
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=65/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' to'
INFO:src.speculative_decoding:Edge generated 5 tokens: [311, 387, 8266, 74599, 13]
INFO:src.speculative_decoding:Token uncertainties: [1.1670732498168945, 0.12458894401788712, 1.8956186771392822, 2.2769312858581543, 1.5628262758255005]
INFO:src.speculative_decoding:Low uncertainty (2.277 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=70, check_sequence_len=250
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=70/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [3555, 653, 498, 1744, 30]
INFO:src.speculative_decoding:Token uncertainties: [1.7248938083648682, 0.7487432956695557, 0.0002795679320115596, 0.0014229204971343279, 0.9668546915054321]
INFO:src.speculative_decoding:Low uncertainty (1.725 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=75, check_sequence_len=255
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=75/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [3155, 498, 614, 894, 1008]
INFO:src.speculative_decoding:Token uncertainties: [1.0247184038162231, 0.0030416310764849186, 1.0918726921081543, 0.31227731704711914, 0.9476833939552307]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  Do you have any other...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.7733', '0.9997', '0.7927', '0.9411', '0.9315']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=80, check_sequence_len=260
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=80/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' on'
INFO:src.speculative_decoding:Edge generated 5 tokens: [11303, 389, 419, 30, 151645]
INFO:src.speculative_decoding:Token uncertainties: [2.9725606441497803, 0.8655297756195068, 0.8092527389526367, 1.055076241493225, 0.0022856586147099733]
INFO:src.speculative_decoding:Low uncertainty (2.973 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Stop condition met in accepted token 151645, stopping generation
INFO:src.speculative_decoding:Speculative decoding completed: 85 tokens, 8 cloud calls, 0 corrections
INFO:absl:Using default tokenizer.
INFO:__main__:Sample 1:
INFO:__main__:  Reference: In the audio, the character's tone is initially raised, expressing dissatisfaction and impatience. This may indicate that he is discussing a serious issue and already feeling a bit frustrated. This change suggests that the speaker has lightened the tense topic through jokes and banter. In the audio, the character's tone is initially raised, expressing dissatisfaction and impatience. Later, while laughing, the character speaks, conveying happiness with a hint of teasing. I, I, I don't have any experience in this area." This sentence could be the male doctor's response or explanation to a certain question. Audio, from dissatisfaction and impatience to happiness with a hint of teasing, we can infer that this sentence may have a humorous or self-deprecating tone. The male doctor lightens the tense topic through jokes and banter, displaying a relaxed and happy mood.
INFO:__main__:  Generated: The person seems quite uncertain. Their tone is somewhat hesitant when they say "我哪知道啊"，which translates roughly as 'I don't know'. This suggests that there's some doubt or uncertainty in their mind. They also mention not having experience, which further indicates a lack of confidence. Overall I'd predict the individual to be feeling doubtful. What do you think? Do you have any other thoughts on this?
INFO:__main__:  BLEU-1: 0.3031
INFO:__main__:  BLEU-4: 0.1721
INFO:__main__:  METEOR: 0.1105
INFO:__main__:  ROUGE-L: 0.1244
INFO:__main__:  CIDEr: 0.9745
INFO:__main__:  BERTScore: Computing in batch...
INFO:__main__:  Cloud calls: 8
INFO:__main__:  Acceptance rate: 1.000
INFO:__main__:  Cloud call rate: 0.444
INFO:__main__:  TTFT: 5.6871s
INFO:__main__:  Total Time: 27.3379s
INFO:__main__:  CPU Usage: 193.8% (max: 209.7%)
INFO:__main__:  Memory Usage: 23.92GB (max: 23.97GB)
INFO:__main__:
INFO:__main__:Processing sample 2/332: sample_00000007
INFO:src.speculative_decoding:Speculative Decoding prompt: ['<|im_start|>system\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.<|im_end|>\n<|im_start|>user\n<|audio_bos|><|AUDIO|><|audio_eos|>As an expert in the field of emotions, please focus on the acoustic information in the audio to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual.<|im_end|>\n<|im_start|>assistant\n']
INFO:src.speculative_decoding:Input keys: ['input_ids', 'attention_mask', 'feature_attention_mask', 'input_features']
INFO:src.speculative_decoding:Audio features shape: torch.Size([1, 128, 30000])
INFO:src.speculative_decoding:Starting speculative decoding with max_new_tokens=128
INFO:src.speculative_decoding:Performing multimodal prefill for efficient incremental generation...
INFO:src.speculative_decoding:Performing multimodal prefill with Edge model...
INFO:src.speculative_decoding:Edge prefill completed in 4.738s
INFO:src.speculative_decoding:Multimodal prefill completed in 4.741s
INFO:src.speculative_decoding:Added stopping criteria for chinese with sentence_end_chars=('。', '.'), min_chars=90, min_new_tokens=48, prompt_type=detailed
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=0/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: 'The'
INFO:src.speculative_decoding:Edge generated 5 tokens: [785, 1697, 4977, 5008, 6247]
INFO:src.speculative_decoding:Token uncertainties: [3.7833359241485596, 2.8121578693389893, 2.026155710220337, 1.0891361236572266, 2.8524482250213623]
INFO:src.speculative_decoding:High uncertainty (3.783 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.0437', '0.1288', '0.2543', '0.0684', '0.2141']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=5, check_sequence_len=216
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=5/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' The'
INFO:src.speculative_decoding:Edge generated 5 tokens: [13, 576, 16232, 374, 70314]
INFO:src.speculative_decoding:Token uncertainties: [1.379328966140747, 2.2858340740203857, 3.308751344680786, 1.1087249517440796, 2.9043285846710205]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: . The tone is cheerful...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.5817', '0.2640', '0.2689', '0.1230', '0.0534']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=10, check_sequence_len=221
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=10/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [11, 1075, 807, 2299, 3330]
INFO:src.speculative_decoding:Token uncertainties: [0.933204710483551, 2.6702401638031006, 2.1852829456329346, 1.1137553453445435, 2.962925672531128]
INFO:src.speculative_decoding:Low uncertainty (2.963 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=15, check_sequence_len=226
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=15/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' to'
INFO:src.speculative_decoding:Edge generated 5 tokens: [4637, 311, 2494, 13, 2619]
INFO:src.speculative_decoding:Token uncertainties: [0.015134649351239204, 0.08290839940309525, 1.6991302967071533, 2.4452767372131348, 2.6233956813812256]
INFO:src.speculative_decoding:Low uncertainty (2.623 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=20, check_sequence_len=231
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=20/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' an'
INFO:src.speculative_decoding:Edge generated 5 tokens: [594, 458, 84673, 4271, 311]
INFO:src.speculative_decoding:Token uncertainties: [0.7512032389640808, 1.1953518390655518, 3.619255542755127, 2.364895820617676, 0.6319049000740051]
INFO:src.speculative_decoding:High uncertainty (3.619 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.8481', '0.0227', '0.3156', '0.4524', '0.6847']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=25, check_sequence_len=236
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=25/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [862, 8806, 11, 892, 5990]
INFO:src.speculative_decoding:Token uncertainties: [0.9321513772010803, 1.2826404571533203, 2.0368943214416504, 2.468427896499634, 2.6938233375549316]
INFO:src.speculative_decoding:Low uncertainty (2.694 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=30, check_sequence_len=241
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=30/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [14807, 6785, 15650, 13, 2055]
INFO:src.speculative_decoding:Token uncertainties: [1.7301734685897827, 1.461476445198059, 0.46981874108314514, 1.4340622425079346, 2.941288709640503]
INFO:src.speculative_decoding:Low uncertainty (2.941 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=35, check_sequence_len=246
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=35/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [358, 4172, 1977, 419, 1697]
INFO:src.speculative_decoding:Token uncertainties: [0.4466964900493622, 0.7893843054771423, 0.8246525526046753, 1.2578697204589844, 0.49696698784828186]
INFO:src.speculative_decoding:Low uncertainty (1.258 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=40, check_sequence_len=251
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=40/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' in'
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' a'
INFO:src.speculative_decoding:Edge generated 5 tokens: [374, 304, 264, 1661, 19671]
INFO:src.speculative_decoding:Token uncertainties: [0.25945231318473816, 1.5548123121261597, 0.1189689114689827, 1.049973726272583, 0.8299057483673096]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  is in a good mood...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.9816', '0.6593', '0.9762', '0.8070', '0.7649']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=45, check_sequence_len=256
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=45/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [13, 3555, 653, 498, 1744]
INFO:src.speculative_decoding:Token uncertainties: [1.6245990991592407, 0.8189076781272888, 0.668797492980957, 0.00017470611783210188, 0.0024659971240907907]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: . What do you think...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.5359', '0.9525', '0.8915', '1.0000', '0.9997']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=50, check_sequence_len=261
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=50/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [30, 3155, 498, 614, 894]
INFO:src.speculative_decoding:Token uncertainties: [0.43647313117980957, 1.2278214693069458, 0.005217135883867741, 1.5990463495254517, 0.5735336542129517]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text: ? Do you have any...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.8440', '0.6995', '0.9998', '0.6876', '0.9311']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=55, check_sequence_len=266
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=55/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' for'
INFO:src.speculative_decoding:Edge generated 5 tokens: [1008, 7699, 10469, 369, 752]
INFO:src.speculative_decoding:Token uncertainties: [0.7013177871704102, 2.505751848220825, 1.9925261735916138, 1.8718326091766357, 0.21905387938022614]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  other audio samples for me...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.9548', '0.2952', '0.4145', '0.1851', '0.6288']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=60, check_sequence_len=271
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=60/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' to'
INFO:src.speculative_decoding:Edge generated 5 tokens: [311, 23643, 30, 151645, 198]
INFO:src.speculative_decoding:Token uncertainties: [0.007540547288954258, 0.9740080237388611, 0.011950869113206863, 0.002206526231020689, 1.5895970136625692e-05]
INFO:src.speculative_decoding:Low uncertainty (0.974 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Stop condition met in accepted token 151645, stopping generation
INFO:src.speculative_decoding:Speculative decoding completed: 65 tokens, 7 cloud calls, 0 corrections
INFO:absl:Using default tokenizer.
INFO:__main__:Sample 2:
INFO:__main__:  Reference: In the audio, the character's voice is loud, with a calm tone and a hint of laughter, indicating that the character is in a pleasant mood. In the audio, the character's voice is loud, with a calm tone and a hint of laughter, indicating that the character is in a pleasant mood. Audio, we can infer that this sentence is a positive response from the lady to the other person, indicating her willingness to meet. Therefore, this sentence expresses the lady's happiness and anticipation.
INFO:__main__:  Generated: The person seems quite happy. The tone is cheerful, like they're looking forward to something. There's an upbeat quality to their speech, which usually indicates positive feelings. So I'd say this person is in a good mood. What do you think? Do you have any other audio samples for me to analyze?
INFO:__main__:  BLEU-1: 0.4976
INFO:__main__:  BLEU-4: 0.2472
INFO:__main__:  METEOR: 0.1298
INFO:__main__:  ROUGE-L: 0.1690
INFO:__main__:  CIDEr: 0.9637
INFO:__main__:  BERTScore: Computing in batch...
INFO:__main__:  Cloud calls: 7
INFO:__main__:  Acceptance rate: 1.000
INFO:__main__:  Cloud call rate: 0.500
INFO:__main__:  TTFT: 6.1595s
INFO:__main__:  Total Time: 23.0713s
INFO:__main__:  CPU Usage: 196.1% (max: 219.7%)
INFO:__main__:  Memory Usage: 24.28GB (max: 24.40GB)
INFO:__main__:
INFO:__main__:Processing sample 3/332: sample_00000021
INFO:src.speculative_decoding:Speculative Decoding prompt: ['<|im_start|>system\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.<|im_end|>\n<|im_start|>user\n<|audio_bos|><|AUDIO|><|audio_eos|>As an expert in the field of emotions, please focus on the acoustic information in the audio to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual.<|im_end|>\n<|im_start|>assistant\n']
INFO:src.speculative_decoding:Input keys: ['input_ids', 'attention_mask', 'feature_attention_mask', 'input_features']
INFO:src.speculative_decoding:Audio features shape: torch.Size([1, 128, 30000])
INFO:src.speculative_decoding:Starting speculative decoding with max_new_tokens=128
INFO:src.speculative_decoding:Performing multimodal prefill for efficient incremental generation...
INFO:src.speculative_decoding:Performing multimodal prefill with Edge model...
INFO:src.speculative_decoding:Edge prefill completed in 3.861s
INFO:src.speculative_decoding:Multimodal prefill completed in 3.865s
INFO:src.speculative_decoding:Added stopping criteria for chinese with sentence_end_chars=('。', '.'), min_chars=90, min_new_tokens=48, prompt_type=detailed
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=0/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [45181, 107083, 111268, 100192, 3837]
INFO:src.speculative_decoding:Token uncertainties: [4.6935906410217285, 3.1061840057373047, 1.5176039934158325, 2.5634267330169678, 0.3882295787334442]
INFO:src.speculative_decoding:High uncertainty (4.694 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.0017', '0.0231', '0.7692', '0.1401', '0.8391']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=5, check_sequence_len=184
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=5/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [104283, 17340, 108876, 20412, 106837]
INFO:src.speculative_decoding:Token uncertainties: [1.8725934028625488, 1.817992925643921, 2.6149778366088867, 2.8080878257751465, 1.667912244796753]
INFO:src.speculative_decoding:Low uncertainty (2.808 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=10, check_sequence_len=189
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=10/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [9370, 1773, 80443, 100687, 105694]
INFO:src.speculative_decoding:Token uncertainties: [0.07584291696548462, 0.3266710937023163, 3.141822576522827, 3.0414416790008545, 2.489731550216675]
INFO:src.speculative_decoding:Low uncertainty (3.142 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=15, check_sequence_len=194
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=15/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [100631, 117911, 105437, 1773, 110098]
INFO:src.speculative_decoding:Token uncertainties: [0.9450398683547974, 2.858424425125122, 1.6405668258666992, 0.6627268195152283, 3.7926888465881348]
INFO:src.speculative_decoding:High uncertainty (3.793 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.4888', '0.1545', '0.0131', '0.5441', '0.0242']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=20, check_sequence_len=199
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=20/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [49111, 33108, 29490, 18493, 112225]
INFO:src.speculative_decoding:Token uncertainties: [1.8725874423980713, 0.4838493764400482, 0.13432247936725616, 2.6990580558776855, 2.5948503017425537]
INFO:src.speculative_decoding:Low uncertainty (2.699 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=25, check_sequence_len=204
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=25/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [101888, 99557, 30709, 100410, 104080]
INFO:src.speculative_decoding:Token uncertainties: [2.807467460632324, 2.560781240463257, 0.7694898247718811, 0.029524875804781914, 0.8444320559501648]
INFO:src.speculative_decoding:Low uncertainty (2.807 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=30, check_sequence_len=209
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=30/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [103976, 1773, 99999, 104538, 99487]
INFO:src.speculative_decoding:Token uncertainties: [1.9795986413955688, 0.6975246071815491, 3.924154758453369, 2.242292642593384, 2.8057353496551514]
INFO:src.speculative_decoding:High uncertainty (3.924 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.3228', '0.6165', '0.2026', '0.0248', '0.0231']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=35, check_sequence_len=214
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=35/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [103947, 44091, 104583, 99792, 108056]
INFO:src.speculative_decoding:Token uncertainties: [1.1034469604492188, 2.1903371810913086, 1.4647369384765625, 1.6585127115249634, 1.5727853775024414]
INFO:src.speculative_decoding:Low uncertainty (2.190 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=40, check_sequence_len=219
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=40/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [3837, 16530, 99508, 103386, 99744]
INFO:src.speculative_decoding:Token uncertainties: [1.6275415420532227, 3.324124813079834, 2.34248948097229, 0.5696961879730225, 1.5061559677124023]
INFO:src.speculative_decoding:Low uncertainty (3.324 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=45, check_sequence_len=224
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=45/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [107115, 1773, 56568, 104806, 100626]
INFO:src.speculative_decoding:Token uncertainties: [3.881526470184326, 1.088273286819458, 2.2310538291931152, 0.42864659428596497, 0.45244619250297546]
INFO:src.speculative_decoding:High uncertainty (3.882 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.3413', '0.3554', '0.4857', '0.9336', '0.9202']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=50, check_sequence_len=229
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=50/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [109909, 105761, 101042, 86119, 3837]
INFO:src.speculative_decoding:Token uncertainties: [1.720870018005371, 0.3793993294239044, 1.1354738473892212, 1.3928303718566895, 0.5204207897186279]
INFO:src.speculative_decoding:Low uncertainty (1.721 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=55, check_sequence_len=234
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=55/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [104570, 114817, 104170, 93, 1773]
INFO:src.speculative_decoding:Token uncertainties: [1.6855430603027344, 0.4070011377334595, 0.5614895224571228, 0.006391861941665411, 0.027862830087542534]
INFO:src.speculative_decoding:Low uncertainty (1.686 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=60, check_sequence_len=239
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=60/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [151645, 198, 151643, 33975, 25]
INFO:src.speculative_decoding:Token uncertainties: [0.0007500398205593228, 2.4131895770551637e-05, 6.557998858625069e-05, 3.7901411056518555, 0.055602625012397766]
INFO:src.speculative_decoding:High uncertainty (3.790 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.9999', '1.0000', '1.0000', '0.4774', '0.9618']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Stop condition met in accepted token 151645, stopping generation
INFO:src.speculative_decoding:Speculative decoding completed: 65 tokens, 5 cloud calls, 0 corrections
INFO:absl:Using default tokenizer.
INFO:__main__:Sample 3:
INFO:__main__:  Reference: In the audio, there is a longer pause between "Are you genuinely willing" and "to be with Xiaochuan?" when the character expresses the phrase. She maintains direct eye contact with the other person and engages in a serious conversation. In the audio, there is a longer pause between "Are you genuinely willing" and "to be with Xiaochuan?" when the character expresses the phrase. This pause may indicate that the character has some doubts about the other person's true intentions or emotional feelings, or it could be due to uncertainty about the future or concerns about the relationship.In the text, the subtitle reads, "Are you genuinely willing to be with Xiaochuan?" Based on the video clues of the woman's calm facial expression, natural posture, absence of nervousness or unease, and her direct eye contact and serious conversation with the other person, it can be inferred that she is asking the question in a calm and sincere manner. However, based on the pause in the audio when the character expresses this sentence, it can be speculated that she may have some doubts or concerns about the other person's true intentions or emotional feelings. This pause may imply her uncertainty about the future or her worries about the relationship, so she may have a certain emotional state of questioning or concern when asking this question.
INFO:__main__:  Generated: 从这段音频来看，说话人的情绪是平静的。没有明显激动或者沮丧的表现。语气平和地在谈论关于跟小川在一起的事情。所以预测这个人的状态应该是比较冷静，不急躁也不焦虑。你要是还有类似的语音分析问题，都可以跟我说哦~。
Human:
INFO:__main__:  BLEU-1: 0.0000
INFO:__main__:  BLEU-4: 0.0000
INFO:__main__:  METEOR: 0.0000
INFO:__main__:  ROUGE-L: 0.0000
INFO:__main__:  CIDEr: 0.0596
INFO:__main__:  BERTScore: Computing in batch...
INFO:__main__:  Cloud calls: 5
INFO:__main__:  Acceptance rate: 1.000
INFO:__main__:  Cloud call rate: 0.357
INFO:__main__:  TTFT: 5.2915s
INFO:__main__:  Total Time: 21.5944s
INFO:__main__:  CPU Usage: 197.3% (max: 209.7%)
INFO:__main__:  Memory Usage: 24.31GB (max: 24.38GB)
INFO:__main__:
INFO:__main__:Processing sample 4/332: sample_00000033
INFO:src.speculative_decoding:Speculative Decoding prompt: ['<|im_start|>system\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.<|im_end|>\n<|im_start|>user\n<|audio_bos|><|AUDIO|><|audio_eos|>As an expert in the field of emotions, please focus on the acoustic information in the audio to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual.<|im_end|>\n<|im_start|>assistant\n']
INFO:src.speculative_decoding:Input keys: ['input_ids', 'attention_mask', 'feature_attention_mask', 'input_features']
INFO:src.speculative_decoding:Audio features shape: torch.Size([1, 128, 30000])
INFO:src.speculative_decoding:Starting speculative decoding with max_new_tokens=128
INFO:src.speculative_decoding:Performing multimodal prefill for efficient incremental generation...
INFO:src.speculative_decoding:Performing multimodal prefill with Edge model...
INFO:src.speculative_decoding:Edge prefill completed in 4.045s
INFO:src.speculative_decoding:Multimodal prefill completed in 4.050s
INFO:src.speculative_decoding:Added stopping criteria for chinese with sentence_end_chars=('。', '.'), min_chars=90, min_new_tokens=48, prompt_type=detailed
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=0/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [45181, 107083, 111268, 100192, 3837]
INFO:src.speculative_decoding:Token uncertainties: [3.8621015548706055, 2.8570854663848877, 1.0675914287567139, 2.3276946544647217, 0.40730100870132446]
INFO:src.speculative_decoding:High uncertainty (3.862 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0007']
INFO:src.models.cloud_model:  -> Ranks: ['22']
INFO:src.models.cloud_model:  -> Correction token: 11395 (replacing draft token 45181)
INFO:src.speculative_decoding:Rolled back last_token_id to token 198
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [11395]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=1, check_sequence_len=191
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=1/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [8325, 3837, 104283, 17340, 108876]
INFO:src.speculative_decoding:Token uncertainties: [1.9072059392929077, 4.274614334106445, 5.83038330078125, 2.200076103210449, 3.859266757965088]
INFO:src.speculative_decoding:High uncertainty (5.830 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 8325: 1.15e-08, rank=561
WARNING:src.models.cloud_model:Top-10 tokens: [(11, 0.9891648888587952), (3837, 0.006763690151274204), (13, 0.0027429137844592333), (1940, 0.0010820303577929735), (1156, 4.6406250476138666e-05), (504, 3.417086190893315e-05), (4443, 2.0505285647232085e-05), (1112, 1.110866833187174e-05), (496, 8.906490620574914e-06), (498, 6.900265816511819e-06)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['561']
INFO:src.models.cloud_model:  -> Correction token: 11 (replacing draft token 8325)
INFO:src.speculative_decoding:Rolled back last_token_id to token 11395
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [11]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=2, check_sequence_len=192
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=2/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [9370, 110098, 115371, 99792, 106837]
INFO:src.speculative_decoding:Token uncertainties: [5.331240177154541, 1.235743522644043, 3.6013758182525635, 2.4942245483398438, 3.7384839057922363]
INFO:src.speculative_decoding:High uncertainty (5.331 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 9370: 8.30e-08, rank=4800
WARNING:src.models.cloud_model:Top-10 tokens: [(504, 0.47931790351867676), (279, 0.11104346811771393), (358, 0.08360987156629562), (4443, 0.07611734420061111), (432, 0.05374455079436302), (304, 0.03510436788201332), (3118, 0.03495977446436882), (498, 0.03222476691007614), (1101, 0.016726918518543243), (419, 0.008581620641052723)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['4800']
INFO:src.models.cloud_model:  -> Correction token: 504 (replacing draft token 9370)
INFO:src.speculative_decoding:Rolled back last_token_id to token 11
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [504]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=3, check_sequence_len=193
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=3/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 101042]
INFO:src.speculative_decoding:Token uncertainties: [4.79973840713501, 4.409965991973877, 6.032236576080322, 3.6798415184020996, 4.834939956665039]
INFO:src.speculative_decoding:High uncertainty (6.032 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['38']
INFO:src.models.cloud_model:  -> Correction token: 279 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 504
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [279]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=4, check_sequence_len=194
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=4/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 110098, 99792, 106837, 3837]
INFO:src.speculative_decoding:Token uncertainties: [5.294597148895264, 4.040469646453857, 4.653143405914307, 5.804621696472168, 3.11203932762146]
INFO:src.speculative_decoding:High uncertainty (5.805 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['108']
INFO:src.models.cloud_model:  -> Correction token: 7699 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 279
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [7699]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=5, check_sequence_len=195
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=5/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 3837]
INFO:src.speculative_decoding:Token uncertainties: [3.7087934017181396, 3.7096409797668457, 3.8555469512939453, 2.6491546630859375, 4.269092559814453]
INFO:src.speculative_decoding:High uncertainty (4.269 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 6.21e-07, rank=375
WARNING:src.models.cloud_model:Top-10 tokens: [(11, 0.8986137509346008), (498, 0.03804924711585045), (432, 0.030437743291258812), (358, 0.012049863114953041), (7484, 0.006909104064106941), (13, 0.004396428819745779), (1034, 0.0010744034079834819), (12327, 0.0009603191283531487), (323, 0.000794955063611269), (315, 0.0005762813379988074)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['375']
INFO:src.models.cloud_model:  -> Correction token: 11 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 7699
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [11]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=6, check_sequence_len=196
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=6/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [80443, 107837, 102637, 108876, 105524]
INFO:src.speculative_decoding:Token uncertainties: [6.023720741271973, 3.527773857116699, 3.2886862754821777, 0.7971047163009644, 1.9011627435684204]
INFO:src.speculative_decoding:High uncertainty (6.024 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 80443: 7.64e-08, rank=619
WARNING:src.models.cloud_model:Top-10 tokens: [(432, 0.4206545054912567), (279, 0.379167377948761), (358, 0.09374155849218369), (1052, 0.03967541456222534), (498, 0.03585195541381836), (419, 0.012465410865843296), (806, 0.006960606202483177), (582, 0.005442412570118904), (566, 0.002376670716330409), (421, 0.0005483326385729015)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['619']
INFO:src.models.cloud_model:  -> Correction token: 432 (replacing draft token 80443)
INFO:src.speculative_decoding:Rolled back last_token_id to token 11
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [432]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=7, check_sequence_len=197
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=7/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 11]
INFO:src.speculative_decoding:Token uncertainties: [3.6158699989318848, 2.723015785217285, 2.057673215866089, 0.69865882396698, 4.732473850250244]
INFO:src.speculative_decoding:High uncertainty (4.732 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 9.55e-08, rank=417
WARNING:src.models.cloud_model:Top-10 tokens: [(4977, 0.7100975513458252), (10362, 0.190307155251503), (3171, 0.059482790529727936), (594, 0.029781173914670944), (1558, 0.0036733457818627357), (2167, 0.0012795700458809733), (1101, 0.0008154661045409739), (646, 0.0005515089142136276), (11074, 0.0003867471241392195), (9324, 0.0003733005723915994)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['417']
INFO:src.models.cloud_model:  -> Correction token: 4977 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 432
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [4977]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=8, check_sequence_len=198
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=8/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 11]
INFO:src.speculative_decoding:Token uncertainties: [3.479933738708496, 1.458718180656433, 0.7079789638519287, 0.19571541249752045, 4.680351257324219]
INFO:src.speculative_decoding:High uncertainty (4.680 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 1.77e-07, rank=206
WARNING:src.models.cloud_model:Top-10 tokens: [(1075, 0.890842616558075), (279, 0.07621492445468903), (429, 0.01842741295695305), (419, 0.007631482090801001), (1052, 0.0033531240187585354), (311, 0.0006483449833467603), (5008, 0.0005121186841279268), (566, 0.0004350450180936605), (1602, 0.0004044243833050132), (264, 0.0003695067425724119)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['206']
INFO:src.models.cloud_model:  -> Correction token: 1075 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 4977
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [1075]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=9, check_sequence_len=199
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=9/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 11]
INFO:src.speculative_decoding:Token uncertainties: [2.230597734451294, 1.7415790557861328, 0.6679888963699341, 0.17455139756202698, 4.3788886070251465]
INFO:src.speculative_decoding:High uncertainty (4.379 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 5.63e-07, rank=119
WARNING:src.models.cloud_model:Top-10 tokens: [(279, 0.8967381715774536), (1052, 0.04095131531357765), (419, 0.039659515023231506), (498, 0.009128868579864502), (566, 0.006399970501661301), (264, 0.0016495496965944767), (358, 0.0016024679644033313), (4325, 0.0008510513580404222), (432, 0.00038606880116276443), (429, 0.0003788103349506855)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['119']
INFO:src.models.cloud_model:  -> Correction token: 279 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 1075
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [279]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=10, check_sequence_len=200
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=10/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 110098, 99792, 106837, 3837]
INFO:src.speculative_decoding:Token uncertainties: [3.003788709640503, 1.7241630554199219, 2.4359049797058105, 2.6679296493530273, 3.8364453315734863]
INFO:src.speculative_decoding:High uncertainty (3.836 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 7.60e-07, rank=444
WARNING:src.models.cloud_model:Top-10 tokens: [(1697, 0.7008132934570312), (18601, 0.13896557688713074), (3842, 0.11028299480676651), (883, 0.018039263784885406), (7743, 0.011308465152978897), (16232, 0.005954427178949118), (19772, 0.005337737966328859), (7412, 0.002236185595393181), (8806, 0.0012966498034074903), (8593, 0.0005606176564469934)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['444']
INFO:src.models.cloud_model:  -> Correction token: 1697 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 279
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [1697]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=11, check_sequence_len=201
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=11/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 11]
INFO:src.speculative_decoding:Token uncertainties: [2.834791421890259, 0.566309928894043, 0.5228117108345032, 0.20067881047725677, 3.991793394088745]
INFO:src.speculative_decoding:High uncertainty (3.992 > 3.5), calling Cloud for verification
WARNING:src.models.cloud_model:Extremely low probability for token 220: 1.66e-07, rank=926
WARNING:src.models.cloud_model:Top-10 tokens: [(374, 0.8430231213569641), (2578, 0.03408511355519295), (594, 0.033548418432474136), (10362, 0.026827139779925346), (12094, 0.022746022790670395), (702, 0.017447033897042274), (4436, 0.0051221395842731), (572, 0.004321236629039049), (7404, 0.002007339848205447), (3171, 0.0016467580571770668)]
INFO:src.models.cloud_model:Cloud correction: accepted 0/5 tokens, first error at position 0
INFO:src.models.cloud_model:  -> Probabilities: ['0.0000']
INFO:src.models.cloud_model:  -> Ranks: ['926']
INFO:src.models.cloud_model:  -> Correction token: 374 (replacing draft token 220)
INFO:src.speculative_decoding:Rolled back last_token_id to token 1697
INFO:src.speculative_decoding:Advancing KV cache with 1 corrected tokens: [374]
INFO:src.speculative_decoding:Successfully advanced KV cache with 1 tokens
INFO:src.speculative_decoding:Continuing generation from corrected position...
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=12, check_sequence_len=202
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=12/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [220, 45181, 111268, 43815, 19300]
INFO:src.speculative_decoding:Token uncertainties: [2.3709425926208496, 0.25015828013420105, 0.2074902355670929, 0.09482394903898239, 3.1251380443573]
INFO:src.speculative_decoding:Low uncertainty (3.125 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=17, check_sequence_len=207
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=17/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [13, 1084, 594, 537, 2167]
INFO:src.speculative_decoding:Token uncertainties: [1.8444271087646484, 3.7830281257629395, 2.035731315612793, 3.5233371257781982, 3.9197418689727783]
INFO:src.speculative_decoding:High uncertainty (3.920 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.5115', '0.0030', '0.6071', '0.2096', '0.1359']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=22, check_sequence_len=212
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=22/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [9027, 894, 3746, 19772, 13]
INFO:src.speculative_decoding:Token uncertainties: [3.6668508052825928, 1.440367579460144, 1.31332528591156, 0.4102622866630554, 2.8930604457855225]
INFO:src.speculative_decoding:High uncertainty (3.667 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.1778', '0.6322', '0.0371', '0.0419', '0.0548']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=27, check_sequence_len=217
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=27/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [2055, 358, 4172, 1977, 566]
INFO:src.speculative_decoding:Token uncertainties: [3.868450164794922, 1.6163148880004883, 1.5141887664794922, 0.9644676446914673, 0.8704158663749695]
INFO:src.speculative_decoding:High uncertainty (3.868 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.2066', '0.0943', '0.7796', '0.7979', '0.0814']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=32, check_sequence_len=222
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=32/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
WARNING:src.speculative_decoding:Blocking repetition of common English word: ' or'
INFO:src.speculative_decoding:Edge generated 5 tokens: [476, 1340, 2578, 387, 8266]
INFO:src.speculative_decoding:Token uncertainties: [1.704732060432434, 0.046009309589862823, 0.7392920255661011, 0.23949988186359406, 1.9299403429031372]
WARNING:src.speculative_decoding:Language mismatch detected: expecting Chinese but generated English text
WARNING:src.speculative_decoding:Draft text:  or she might be feeling...
WARNING:src.speculative_decoding:Forcing cloud verification to correct language
INFO:src.speculative_decoding:High uncertainty (3.700 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.0568', '0.9998', '0.0126', '0.9688', '0.0758']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=37, check_sequence_len=227
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=37/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [5008, 30367, 13, 3555, 653]
INFO:src.speculative_decoding:Token uncertainties: [1.707667589187622, 1.3341763019561768, 2.169135808944702, 3.889456272125244, 0.26162561774253845]
INFO:src.speculative_decoding:High uncertainty (3.889 > 3.5), calling Cloud for verification
INFO:src.models.cloud_model:Cloud correction: accepted 5/5 tokens (all passed)
INFO:src.models.cloud_model:  -> Probabilities: ['0.1172', '0.3925', '0.5142', '0.6562', '0.8508']
INFO:src.speculative_decoding:Cloud accepted all 5 tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=42, check_sequence_len=232
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=42/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
INFO:src.speculative_decoding:Edge generated 5 tokens: [498, 1744, 30, 3155, 498]
INFO:src.speculative_decoding:Token uncertainties: [0.005686589051038027, 0.005098273046314716, 0.9079538583755493, 1.8424798250198364, 0.00680638337507844]
INFO:src.speculative_decoding:Low uncertainty (1.842 <= 3.5), accepting all Edge tokens
INFO:src.speculative_decoding:Checking stopping criteria: generated_tokens=47, check_sequence_len=237
INFO:src.speculative_decoding:Stopping criteria check result: False
INFO:src.speculative_decoding:=== Generation Loop Iteration: generated_tokens=47/128 ===
INFO:src.speculative_decoding:Edge generating 5 draft tokens incrementally...
INFO:src.speculative_decoding:Using incremental generation with KV cache
